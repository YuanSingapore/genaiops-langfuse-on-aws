{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Langfuse Tracing\n",
    "\n",
    "In this lab, we will learn how to use Langfuse tracing to log and analyze the execution of your LLM applications. The Langfuse supports self-hosted on AWS (this lab) and there is a [cloud version](https://cloud.langfuse.com/) available. [Tracing](https://langfuse.com/docs/tracing) in Langfuse is a way to log and analyze the execution of your LLM applications and following reference provides a detailed overview of the data model used. It is inspired by OpenTelemetry.\n",
    "\n",
    "\n",
    "## [Traces and Observations](https://langfuse.com/docs/tracing-data-model)\n",
    "A trace typically represents a single request or operation. It contains the overall input and output of the function, as well as metadata about the request, such as the user, the session, and tags. Usually, a trace corresponds to a single api call of an application.\n",
    "\n",
    "Each trace can contain multiple observations to log the individual steps of the execution.\n",
    "\n",
    "- Observations are of different types:\n",
    "    - Events are the basic building blocks. They are used to track discrete events in a trace.\n",
    "    - Spans represent durations of units of work in a trace.\n",
    "    - Generations are spans used to log generations of AI models. They contain additional attributes about the model, the prompt, and the completion. For generations, [token usage and costs](https://langfuse.com/docs/model-usage-and-cost) are automatically calculated.\n",
    "- Observations can be nested.\n",
    "\n",
    "![Trace and Observations](./images/trace-observation.png)\n",
    "![Trace and Observations UI](./images/trace-observation-ui.png)\n",
    "\n",
    "## [Sessions](https://langfuse.com/docs/tracing-data-model)\n",
    "Optionally, traces can be grouped into sessions. Sessions are used to group traces that are part of the same user interaction. A common example is a thread in a chat interface.\n",
    "Please refer to the [Sessions documentation](https://langfuse.com/docs/sessions) to add sessions to your traces.\n",
    "\n",
    "![Trace and Sessions](./images/trace-sessions.png)\n",
    "![Trace and Sessions UI](./images/trace-sessions-ui.png)\n",
    "\n",
    "\n",
    "## [Scores](https://langfuse.com/docs/tracing-data-model)\n",
    "\n",
    "Traces and observations can be evaluated using [scores](https://langfuse.com/docs/scores/overview). Scores are flexible objects that store evaluation metrics and can be:\n",
    "\n",
    "- Numeric, categorical, or boolean values\n",
    "- Associated with a trace (required)\n",
    "- Linked to a specific observation (optional)\n",
    "- Annotated with comments for additional context\n",
    "- Validated against a score configuration schema (optional)\n",
    "\n",
    "![Trace and Scores](./images/trace-scores.png)\n",
    "\n",
    "Please refer to the [scores documentation](https://langfuse.com/docs/scores/overview) to get started. For more details on score types and attributes, refer to the [score data model documentation](https://langfuse.com/docs/scores/data-model).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Scores\n",
    "\n",
    "Traces and observations can be evaluated using [scores](https://langfuse.com/docs/scores/overview). Scores are flexible objects that store evaluation metrics and can be:\n",
    "\n",
    "- Numeric, categorical, or boolean values\n",
    "- Associated with a trace (required)\n",
    "- Linked to a specific observation (optional)\n",
    "- Annotated with comments for additional context\n",
    "- Validated against a score configuration schema (optional)\n",
    "\n",
    "![Trace and Scores](./images/trace-scores.png)\n",
    "\n",
    "[Source](https://langfuse.com/docs/scores/overview)\n",
    "\n",
    "Please refer to the [scores documentation](https://langfuse.com/docs/scores/overview) to get started. For more details on score types and attributes, refer to the [score data model documentation](https://langfuse.com/docs/scores/data-model).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "\n",
    "> If you haven't selected the kernel, please click on the \"Select Kernel\" button at the upper right corner, select Python Environments and choose \".venv (Python 3.9.20) .venv/bin/python Recommended\".\n",
    "\n",
    "> To execute each notebook cell, press Shift + Enter.\n",
    "\n",
    "> ℹ️ You can **skip these prerequisite steps** if you're in an instructor-led workshop using temporary accounts provided by AWS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies and Environment Variables\n",
    "\n",
    "We will use the langfuse and boto3:\n",
    "- The Langfuse Python SDK along with the self-hosting deployment to debug and improve LLM applications by tracing model invocations, managing prompts / models configurations and running evaluations.\n",
    "- The boto3 SDK to interact with models on Amazon Bedrock or Amazon SageMaker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following command to install the required Python SDKs.\n",
    "> Please feel free to skip the installation if you are using a provided AWS account in a AWS organised event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aiobotocore 2.12.2 requires botocore<1.34.52,>=1.34.41, but you have botocore 1.40.11 which is incompatible.\n",
      "sagemaker-jupyterlab-extension-common 0.1.15 requires pydantic==1.*, but you have pydantic 2.11.7 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q langfuse==2.59.7 boto3  --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please make sure you have completed the prerequisites to setup the Langfuse project and API keys in the .env file to connect to self-hosted or cloud Langfuse environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langfuse\n",
      "Version: 2.59.7\n",
      "Summary: A client library for accessing langfuse\n",
      "Home-page: \n",
      "Author: langfuse\n",
      "Author-email: developers@langfuse.com\n",
      "License: MIT\n",
      "Location: /opt/conda/lib/python3.10/site-packages\n",
      "Requires: anyio, backoff, httpx, idna, packaging, pydantic, requests, wrapt\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip show langfuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [Langfuse documentation](https://langfuse.com/docs/get-started) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization and Authentication Check\n",
    "Run the following cells to initialize common libraries and clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Fetched defaults config from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "SageMaker Execution Role: arn:aws:iam::369776982489:role/service-role/AmazonSageMaker-ExecutionRole-20230719T182335\n"
     ]
    }
   ],
   "source": [
    "# import all the necessary packages\n",
    "import sys\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))  # Add parent directory to path\n",
    "from utils import *\n",
    "\n",
    "import boto3, sagemaker \n",
    "from langfuse import Langfuse\n",
    "from langfuse.decorators import langfuse_context, observe\n",
    "from langfuse.model import PromptClient\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "print(f\"SageMaker Execution Role: {role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create bedrock client and bedrock runtime client and please make sure the region is us-west-2 for this lab. The expected result is to see the following output:\n",
    "\n",
    "```\n",
    "Found Nova model: US Nova Pro - us.amazon.nova-pro-v1:0\n",
    "Found Nova model: US Nova Lite - us.amazon.nova-lite-v1:0\n",
    "Found Nova model: US Nova Micro - us.amazon.nova-micro-v1:0\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "> As Nova models in us-west-2 can only be called via Cross-Region Inference (CRIS), the model_id has \"us.\" prefix to indicate this is a CRIS call. This can add latency to the model call.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get keys for your project from the LangFuse settings page: https://cloud.langfuse.com\n",
    "import os\n",
    "secret=get_secret()\n",
    "secrets=json.loads(secret)\n",
    "os.environ['LANGFUSE_PUBLIC_KEY'] = secrets[\"LANGFUSE_PUBLIC_KEY\"]\n",
    "os.environ['LANGFUSE_SECRET_KEY'] = secrets[\"LANGFUSE_SECRET_KEY\"]\n",
    "os.environ['LANGFUSE_HOST'] = secrets['LANGFUSE_HOST'] #, 'https://cloud.langfuse.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Nova Pro is accessible\n",
      "Hello! Yes, I can hear you in the sense that I can read and respond to your text inputs. How can I assist you today? If you have a question or need information on a particular topic, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "bedrock_runtime = boto3.client('bedrock-runtime', region_name='us-east-1')\n",
    "\n",
    "try:\n",
    "    response = bedrock_runtime.converse(\n",
    "        modelId='us.amazon.nova-pro-v1:0',\n",
    "        \n",
    "        messages=[\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': [{'text': 'Hello, can you hear me?'}]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    print(\"Success! Nova Pro is accessible\")\n",
    "    print(response['output']['message']['content'][0]['text'])\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Direct Bedrock call failed: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Nova model: US Nova Pro: us.amazon.nova-pro-v1:0\n"
     ]
    }
   ],
   "source": [
    "# used to access Bedrock configuration\n",
    "# region has to be in us-west-2 for this lab\n",
    "bedrock = boto3.client(service_name=\"bedrock\", region_name=\"us-east-1\")\n",
    "\n",
    "# Check if Nova models are available in this region\n",
    "models = bedrock.list_inference_profiles()\n",
    "nova_found = False\n",
    "for model in models[\"inferenceProfileSummaries\"]:\n",
    "    if (\n",
    "        \"Nova Pro\" in model[\"inferenceProfileName\"]\n",
    "        or \"Nova Lite\" in model[\"inferenceProfileName\"]\n",
    "        or \"Nova Micro\" in model[\"inferenceProfileName\"]\n",
    "    ):\n",
    "        print(\n",
    "            f\"Found Nova model: {model['inferenceProfileName']}: {model['inferenceProfileId']}\"\n",
    "        )\n",
    "        nova_found = True\n",
    "if not nova_found:\n",
    "    raise ValueError(\n",
    "        \"No Nova models found in available models. Please ensure you have access to Nova models.\"\n",
    "    )\n",
    "#  Coverage, log level, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the Langfuse client and check credentials are valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langfuse has been set up correctly\n",
      "You can access your Langfuse instance at: http://langfu-loadb-w7udau70n77l-1274730217.ap-southeast-1.elb.amazonaws.com\n"
     ]
    }
   ],
   "source": [
    "from langfuse import Langfuse\n",
    "\n",
    "# langfuse client\n",
    "langfuse = Langfuse()\n",
    "if langfuse.auth_check():\n",
    "    print(\"Langfuse has been set up correctly\")\n",
    "    print(f\"You can access your Langfuse instance at: {os.environ['LANGFUSE_HOST']}\")\n",
    "else:\n",
    "    print(\n",
    "        \"Credentials not found or invalid. Check your Langfuse API key and host in the .env file.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langfuse Wrappers for Bedrock Converse API \n",
    "You can use the Amazon Bedrock Converse API to create conversational applications that send and receive messages to and from an Amazon Bedrock model. For example, you can create a chat bot that maintains a conversation over many turns and uses a persona or tone customization that is unique to your needs, such as a helpful technical support assistant.\n",
    "\n",
    "To use the Converse API, you use the Converse or ConverseStream (for streaming responses) operations to send messages to a model. It is possible to use the existing base inference operations (InvokeModel or InvokeModelWithResponseStream) for conversation applications. However, we recommend using the Converse API as it provides consistent API, that works with all Amazon Bedrock models that support messages. This means you can write code once and use it with different models. Should a model have unique inference parameters, the Converse API also allows you to pass those unique parameters in a model specific structure.\n",
    "\n",
    "For more details, please refer to the [Carry out a conversation with the Converse API operations](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath('..'))  # Add parent directory to path\n",
    "from config import GUARDRAIL_CONFIG, MODEL_CONFIG\n",
    "from utils import converse, converse_tool_use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Examples\n",
    "\n",
    "#### Define a helper function to call the  Converse API wrapper\n",
    "\n",
    "> Please make sure your have setup the Nova custom model pricing per mentioned in  [Langfuse Setup](https://catalog.workshops.aws/genaiops-langfuse/en-US/00-introduction/langfuse-setup) under Introduction section of the workshop studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "@observe(name=\"Simple Chat\")\n",
    "def simple_chat(\n",
    "    model_config: dict,\n",
    "    messages: list,\n",
    "    prompt: PromptClient = None,\n",
    "    use_guardrails: bool = False,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Executes a simple chat interaction using the specified model configuration.\n",
    "\n",
    "    Args:\n",
    "        model_config (dict): Configuration parameters for the chat model.\n",
    "        messages (list): A list of message dictionaries to be processed.\n",
    "        prompt (PromptClient, optional): Optional prompt client for advanced handling.\n",
    "        use_guardrails (bool, optional): When True, applies additional guardrail configurations.\n",
    "\n",
    "    Returns:\n",
    "        dict: The response from the 'converse' function call.\n",
    "    \"\"\"\n",
    "    config = model_config.copy()\n",
    "\n",
    "    \n",
    "    if use_guardrails:\n",
    "        config[\"guardrailConfig\"] = GUARDRAIL_CONFIG\n",
    "    return converse(messages=messages, prompt=prompt, **config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Case 1\n",
    "let's start with a single turn chat use case and use Nova Pro as the default model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nova_pro': {'model_id': 'us.amazon.nova-pro-v1:0', 'inferenceConfig': {'maxTokens': 4096, 'temperature': 0}}, 'nova_lite': {'model_id': 'us.amazon.nova-lite-v1:0', 'inferenceConfig': {'maxTokens': 2048, 'temperature': 0}}, 'nova_micro': {'model_id': 'us.amazon.nova-micro-v1:0', 'inferenceConfig': {'maxTokens': 2048, 'temperature': 0}}}\n",
      "{'model': 'nova_pro', 'response': 'Certainly! Checking in a guest at a luxury resort involves a meticulous and personalized process to ensure a seamless and memorable experience. Here’s a step-by-step guide:\\n\\n### Pre-Arrival\\n1. **Reservation Confirmation**:\\n   - The guest’s reservation is confirmed via email or phone.\\n   - Special requests (dietary preferences, room preferences, etc.) are noted.\\n\\n2. **Pre-Registration**:\\n   - Guests may be sent a digital form to fill out their preferences and details before arrival.\\n\\n### Arrival\\n3. **Welcome at the Entrance**:\\n   - A concierge or greeter welcomes the guest by name as they arrive.\\n   - Luggage is handled by bell staff.\\n\\n### Check-In Process\\n4. **Reception Greeting**:\\n   - The guest is warmly greeted at the front desk.\\n   - The receptionist addresses the guest by name and confirms their reservation.\\n\\n5. **Document Verification**:\\n   - The guest presents identification and credit card for incidentals.\\n   - The receptionist verifies the details against the reservation system.\\n\\n6. **Room Assignment**:\\n   - The receptionist assigns a room based on the guest’s preferences and availability.\\n   - Any upgrades are communicated (if available and applicable).\\n\\n7. **Explanation of Resort Amenities**:\\n   - The receptionist provides a brief overview of resort amenities (spa, dining options, activities, etc.).\\n   - Any special services (like a personal butler or concierge services) are explained.\\n\\n8. **Electronic Key Card**:\\n   - The guest receives an electronic key card for their room.\\n\\n9. **Welcome Gift**:\\n   - A welcome gift (chocolates, a small souvenir, etc.) is often presented.\\n\\n10. **Finalizing Check-In**:\\n    - The guest signs any necessary documents.\\n    - A copy of the resort map and any relevant information brochures are provided.\\n\\n### Post-Check-In\\n11. **Escort to Room** (Optional):\\n    - A staff member may escort the guest to their room, especially if they are unfamiliar with the layout.\\n\\n12. **In-Room Welcome**:\\n    - The room is prepared with turn-down service in the evening.\\n    - Any special requests (flowers, specific bedding arrangements) are attended to.\\n\\n### Ongoing Service\\n13. **Follow-Up**:\\n    - The concierge or guest services may check in with the guest to ensure everything is to their satisfaction.\\n    - Any additional requests or concerns are promptly addressed.\\n\\n### Technology Integration\\n- **Mobile Check-In**: Some luxury resorts offer mobile check-in options where guests can complete the process via an app before arrival.\\n- **Digital Butler**: Some resorts provide a digital concierge service via app for room service, spa bookings, and other requests.\\n\\nBy following these steps, a luxury resort ensures that each guest feels valued, comfortable, and well-attended to from the moment they arrive.', 'statusCode': 200}\n"
     ]
    }
   ],
   "source": [
    "# Decorator to observe and track this function execution in Langfuse\n",
    "@observe(name=\"Single Turn Example\")\n",
    "def chat_single_model(\n",
    "    messages: list, model_type: str = \"nova_pro\", use_guardrails: bool = False\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Execute a single turn chat interaction using one specified Nova model.\n",
    "\n",
    "    Args:\n",
    "        messages (list): The user's input query\n",
    "        model_type (str): The Nova model to use (nova_pro, nova_lite, or nova_micro)\n",
    "        use_guardrails (bool): Whether to apply guardrails to the model invocation\n",
    "\n",
    "    Returns:\n",
    "        dict: Response containing model output and status code\n",
    "    \"\"\"\n",
    "    langfuse_context.update_current_trace(\n",
    "        user_id=\"nova-user-1\",\n",
    "        tags=[\"lab1\", \"single-turn\"],\n",
    "    )\n",
    "    model_config=MODEL_CONFIG[model_type]\n",
    "    print(MODEL_CONFIG)\n",
    "    #model_config['model_id']='amazon.claude-3-sonnet-20240229-v1:0'\n",
    "    \n",
    "    response = simple_chat(\n",
    "        model_config=MODEL_CONFIG[model_type],\n",
    "        messages=messages,\n",
    "        use_guardrails=use_guardrails,\n",
    "    )\n",
    "\n",
    "    return {\"model\": model_type, \"response\": response, \"statusCode\": 200}\n",
    "\n",
    "\n",
    "# Make a sample request to test the chat API\n",
    "# Ask about luxury resort check-in process\n",
    "print(\n",
    "    chat_single_model(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                #\"model_type\":\"us.amazon.nova-pro-v1:0\",\n",
    "                \"content\": \"Explain the process of checking in a guest at a luxury resort, think step by step.\",\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Force immediate sending of the trace data to Langfuse\n",
    "# Rather than waiting for automatic flush\n",
    "langfuse_context.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Langfuse dashboard, you can find the summary of the traces, model costs and model usage.:\n",
      "http://langfu-loadb-w7udau70n77l-1274730217.ap-southeast-1.elb.amazonaws.com\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"In Langfuse dashboard, you can find the summary of the traces, model costs and model usage.:\\n{os.environ['LANGFUSE_HOST']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Langfuse Dashboard](./images/langfuse-dashboard-use-case-1.png)\n",
    "\n",
    "\n",
    "The detailed traced can be found in the **Traces** section and you can click on the trace to see the detailed trace. And there are some key insights in this trace.\n",
    "\n",
    "- input token is 12 and output token is 662, in total there are 675 tokens used\n",
    "\n",
    "- The model used is us.amazon.nova-pro-v1:0\n",
    "\n",
    "- Model parameters are shown such as temperature, max_tokens, etc.\n",
    "\n",
    "- Most importantly, the total cost of this invovation costs $0.002129 \n",
    "\n",
    "![Langfuse Dashboard](./images/langfuse-trace-use-case-1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Case 2\n",
    "This use case demonstrates running a single trace within one session, where we'll execute three distinct observations using different Nova model variants for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'responses': {'nova_pro': 'Certainly! Here’s a detailed, step-by-step process for checking in a guest at a luxury resort:\\n\\n### Pre-Arrival\\n1. **Reservation Confirmation**\\n   - The guest’s reservation is confirmed via email or phone.\\n   - Any special requests (e.g., room preferences, dietary restrictions) are noted.\\n\\n2. **Pre-Registration**\\n   - Some resorts offer online check-in where guests can fill out necessary information beforehand.\\n   - This may include personal details, payment information, and preferences.\\n\\n### Arrival\\n3. **Welcome at the Entrance**\\n   - A valet or bellhop greets the guest, assists with luggage, and provides a warm welcome.\\n   - The guest is directed to the lobby or check-in area.\\n\\n### Check-In Process\\n4. **Greeting at the Front Desk**\\n   - The front desk staff welcomes the guest by name, creating a personalized experience.\\n   - They may offer a refreshing towel or a welcome drink.\\n\\n5. **Verification of Identity**\\n   - The guest presents identification (e.g., passport, driver’s license).\\n   - The reservation is verified using the guest’s name and confirmation number.\\n\\n6. **Review of Reservation Details**\\n   - The front desk staff reviews the reservation details with the guest, including room type, rate, and any special requests.\\n   - Any discrepancies are addressed immediately.\\n\\n7. **Completion of Registration Form**\\n   - The guest may be asked to fill out a registration form, either digitally or on paper.\\n   - This includes emergency contact information, payment details, and any additional requests.\\n\\n8. **Payment and Deposit**\\n   - The resort may require a credit card for incidentals or a deposit.\\n   - Any pre-paid charges are confirmed, and any additional charges (e.g., resort fees) are explained.\\n\\n9. **Room Assignment**\\n   - The guest is informed of their room number and any special features of their room.\\n   - A resort map and information brochure may be provided.\\n\\n10. **Key Card Issuance**\\n    - The front desk staff provides the guest with a key card and explains how to use it.\\n    - They may also provide information on room access and security features.\\n\\n### Post Check-In\\n11. **Luggage Assistance**\\n    - A bellhop or concierge offers to escort the guest to their room and assist with luggage.\\n    - The front desk staff may call ahead to ensure the room is ready for the guest’s arrival.\\n\\n12. **Welcome Amenities**\\n    - Upon arrival at the room, the guest finds welcome amenities such as fruit, chocolates, or a personalized note.\\n    - The room is checked to ensure it meets the resort’s standards and the guest’s requests.\\n\\n13. **Orientation**\\n    - The concierge or front desk staff may offer a brief orientation of the resort’s amenities, including restaurants, spa, gym, and activities.\\n    - They provide information on resort hours, special events, and any exclusive services available.\\n\\n14. **Follow-Up**\\n    - The front desk may check in with the guest later to ensure everything is to their satisfaction.\\n    - Any additional requests or concerns are promptly addressed.\\n\\n### Throughout the Stay\\n15. **Ongoing Communication**\\n    - The resort staff remains available for any assistance or additional requests throughout the guest’s stay.\\n    - Regular check-ins may be made to ensure a pleasant experience.\\n\\nBy following these steps, a luxury resort ensures a seamless and exceptional check-in experience for its guests.', 'nova_lite': 'Certainly! Checking in a guest at a luxury resort involves a series of meticulous steps to ensure a smooth, personalized, and memorable experience. Here’s a step-by-step breakdown of the process:\\n\\n### 1. **Pre-Arrival Preparation**\\n   - **Reservation Confirmation:** The guest’s reservation is confirmed, and all details are verified. This includes the guest’s name, arrival date, departure date, room type, and any special requests or preferences.\\n   - **Guest Information Gathering:** The resort collects necessary information such as passport details, contact information, and payment details.\\n   - **VIP Services:** For high-profile guests or those who have requested special services, additional arrangements such as private transfers, special room amenities, or personalized services are coordinated.\\n\\n### 2. **Arrival**\\n   - **Welcome Desk:** Upon arrival, the guest is greeted by a dedicated concierge or front desk staff. The greeting is warm and personalized, often including the guest’s name.\\n   - **Baggage Handling:** Bellhops or porters assist with luggage, ensuring a seamless transition from the vehicle to the resort.\\n\\n### 3. **Check-In Process**\\n   - **Verification:** The front desk staff verifies the guest’s identity using their reservation details and identification documents.\\n   - **Room Allocation:** The guest is informed about the room type and any upgrades that may be available. If the guest has a preference for a specific room (e.g., ocean view, corner suite), the staff checks availability and makes arrangements accordingly.\\n   - **Room Preparation:** The room is prepared and cleaned to the resort’s high standards. This includes fresh linens, toiletries, and any personalized amenities requested by the guest.\\n   - **Technology Setup:** Any requested technology setups, such as smart room controls, high-speed internet, or entertainment systems, are configured and tested.\\n\\n### 4. **Guest Orientation**\\n   - **Welcome Packet:** The guest receives a welcome packet that includes information about the resort, room amenities, dining options, activities, and emergency contacts.\\n   - **Tour:** A brief tour of the resort is offered, highlighting key amenities such as the spa, fitness center, dining venues, and recreational facilities.\\n   - **Special Requests:** Any special requests or preferences are confirmed and noted in the guest’s profile.\\n\\n### 5. **Payment and Check-In Formalities**\\n   - **Payment Processing:** The guest is presented with the bill, which includes room charges, taxes, and any additional services used. Payment options are provided, including credit card, cash, or resort credits.\\n   - **Loyalty Program:** If the guest is part of a loyalty program, their details are updated, and they may receive special perks or benefits.\\n   - **Key Card Issuance:** The guest is given a key card or digital key for room access. An explanation of the key card’s functions is provided.\\n\\n### 6. **Personalization and Follow-Up**\\n   - **Personalized Touch:** A personalized touch, such as a welcome drink, fruit basket, or handwritten note, is provided to enhance the guest’s experience.\\n   - **Check-In Confirmation:** The guest is thanked for choosing the resort, and any final questions or concerns are addressed.\\n\\n### 7. **Post-Check-In Services**\\n   - **Room Service:** If the guest has requested room service, it is promptly delivered.\\n   - **Follow-Up:** A member of the staff may follow up later to ensure the guest is comfortable and to offer additional services.\\n\\n### 8. **Ongoing Guest Experience**\\n   - **Concierge Services:** Throughout the stay, concierge services are available to assist with any needs or requests, from dining reservations to arranging activities.\\n   - **Feedback Collection:** The guest is periodically asked for feedback on their experience to ensure continuous improvement.\\n\\nBy following these steps, a luxury resort ensures that each guest feels valued, well-cared for, and thoroughly enjoys their stay.', 'nova_micro': 'Checking in a guest at a luxury resort is a meticulous process designed to ensure a seamless and memorable experience from the moment they arrive. Here’s a step-by-step breakdown of the process:\\n\\n### 1. **Pre-Arrival Preparation**\\n   - **Reservation Confirmation:** The resort staff verifies the guest’s reservation details, including special requests, room preferences, and any pre-arrival amenities requested by the guest.\\n   - **Room Preparation:** The room is thoroughly cleaned, decorated, and stocked with luxury amenities such as high-quality toiletries, fresh linens, and any requested items (e.g., bottled water, chocolates).\\n\\n### 2. **Guest Arrival**\\n   - **Arrival Greeting:** A concierge or valet service is often on hand to greet the guest upon arrival. This may include a warm welcome, assistance with luggage, and a brief overview of resort amenities.\\n   - **Transportation:** Luxury resorts often provide a chauffeur service to transport guests from the airport or main entrance to the resort.\\n\\n### 3. **Check-In Process**\\n   - **Registration:** Guests are directed to the check-in desk, where they present their identification and any required documentation (e.g., credit card for incidentals).\\n   - **Data Entry:** The front desk staff enters the guest’s information into the property management system, including special requests and preferences.\\n   - **Room Selection:** The staff selects the guest’s room, ensuring it meets their preferences and any special requests. They may offer a room upgrade if available and suitable.\\n\\n### 4. **Personalized Welcome**\\n   - **Welcome Note:** A personalized welcome note is often left in the room, often with a complimentary item such as a bottle of champagne or a selection of local delicacies.\\n   - **Amenity Presentation:** The staff may present luxury amenities, such as a welcome basket, high-end toiletries, and other personalized items.\\n\\n### 5. **Room Handover**\\n   - **Room Tour:** The guest is escorted to their room, where they are given a detailed tour. The staff explains the room features, including technology, safety features, and how to access resort services.\\n   - **Room Check:** The guest is given the opportunity to check the room for any issues or to confirm that all requested amenities are present.\\n\\n### 6. **Guest Information**\\n   - **Resort Amenities:** The staff provides information about the resort’s amenities, including dining options, spa services, fitness centers, and any scheduled events.\\n   - **Concierge Services:** The concierge is introduced, and the guest is informed about the personalized services available, such as dining reservations, transportation, and local excursions.\\n\\n### 7. **Technology and Connectivity**\\n   - **Wi-Fi and Technology Setup:** The guest is provided with Wi-Fi access details and any necessary technology setup, such as smart room controls.\\n   - **Mobile App:** If available, the resort’s mobile app is introduced, explaining how it can be used for room control, dining reservations, and accessing resort information.\\n\\n### 8. **Final Touches**\\n   - **Feedback Opportunity:** The staff may ask if the guest has any immediate needs or questions and offer to assist with anything required before their stay begins.\\n   - **Thank You:** The staff expresses gratitude for choosing the resort and wishes the guest a pleasant stay.\\n\\n### 9. **Post-Check-In**\\n   - **Follow-Up:** After the initial check-in, the resort staff may follow up via phone or email to ensure the guest is settling in comfortably and to address any immediate concerns.\\n\\nBy following these steps, a luxury resort ensures that each guest experiences a high level of service and comfort from the moment they arrive until their departure.'}, 'statusCode': 200}\n"
     ]
    }
   ],
   "source": [
    "@observe(name=\"Multi-Turn Example\")\n",
    "def chat_compare_models(\n",
    "    messages: list,\n",
    "    model_types: list = [\"nova_pro\", \"nova_lite\", \"nova_micro\"],\n",
    "    use_guardrails: bool = False,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Execute the same query across all Nova models for comparison.\n",
    "\n",
    "    Args:\n",
    "        messages (list): The user's input query\n",
    "        model_types (list): The Nova models to use (nova_pro, nova_lite, or nova_micro)\n",
    "        use_guardrails (bool): Whether to apply guardrails to the model invocation\n",
    "    Returns:\n",
    "        dict: Responses from all models and status code\n",
    "    \"\"\"\n",
    "    langfuse_context.update_current_trace(\n",
    "        user_id=\"nova-user-1\",\n",
    "        session_id=\"model-comparison\",\n",
    "        tags=[\"lab1\", \"model-comparison\"],\n",
    "    )\n",
    "\n",
    "    responses = {}\n",
    "    for model_type in model_types:\n",
    "        responses[model_type] = simple_chat(\n",
    "            model_config=MODEL_CONFIG[model_type],\n",
    "            messages=messages,\n",
    "            use_guardrails=use_guardrails,\n",
    "        )\n",
    "\n",
    "    return {\"responses\": responses, \"statusCode\": 200}\n",
    "\n",
    "\n",
    "# user request\n",
    "print(\n",
    "    chat_compare_models(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Explain the process of checking in a guest at a luxury resort, think step by step.\",\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "langfuse_context.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you can combine multiple observations into one trace and see the cost and usage of each observation. Nova micro has the lowest cost and fastest response time.\n",
    "\n",
    "![langfuse-traces-use-case-2](./images/langfuse-trace-use-case-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Case 3\n",
    "In this case, let's simulate a RAG use case with a dummy retrieval function called retrieve_context, it is a dummy function that returns a static context.\n",
    "We will simply reuse the simple_chat function to chat with the model by passing the context from the retrieval function as part of the system prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': \"The weather in Sydney on 1st January 2025, with a temperature of 24 degrees Celsius, is quite pleasant. Here are a few comments:\\n\\n1. **Comfortable Temperature**: 24 degrees Celsius is generally considered a comfortable temperature for most people. It's neither too hot nor too cold, making it ideal for outdoor activities.\\n\\n2. **Summer Vibes**: In Sydney, January is a summer month. A temperature of 24 degrees Celsius is typical for this time of year, though it can often be warmer. This suggests a mild summer day.\\n\\n3. **Outdoor Activities**: This temperature is perfect for enjoying Sydney’s beautiful beaches, parks, and outdoor attractions. It’s great for walking, cycling, or simply relaxing outdoors.\\n\\n4. **Clothing**: Light and breathable clothing would be appropriate. A t-shirt, shorts, and perhaps a light jacket or sweater for the evening would be comfortable.\\n\\n5. **Comparison**: Compared to New York and Tokyo, which are significantly cooler at 13 and 11 degrees Celsius respectively, Sydney’s weather is much warmer and more inviting for those who prefer warmer climates.\\n\\nOverall, the weather in Sydney on this day is very enjoyable and conducive to a range of activities.\", 'statusCode': 200}\n"
     ]
    }
   ],
   "source": [
    "CONTEXT = \"\"\"1st January 2025\n",
    "Sydney: 24 degrees celcius.\n",
    "New York: 13 degrees celcius.\n",
    "Tokyo: 11 degrees celcius.\"\"\"\n",
    "\n",
    "\n",
    "@observe(name=\"Dummy Retrieval\")\n",
    "def retrieve_context(query: str) -> str:\n",
    "    \"\"\"Retrieves static context for the given query.\"\"\"\n",
    "    return CONTEXT\n",
    "\n",
    "\n",
    "@observe(name=\"RAG Example\")\n",
    "def rag_api(query: str) -> dict:\n",
    "    \"\"\"\n",
    "    Performs a Retrieval-Augmented Generation (RAG) query using a static context.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user query.\n",
    "\n",
    "    Returns:\n",
    "        dict: The response from the model and a status code.\n",
    "    \"\"\"\n",
    "    langfuse_context.update_current_trace(\n",
    "        user_id=\"nova-user-1\",\n",
    "        session_id=\"rag-session\",\n",
    "        tags=[\"lab1\", \"rag-example\"],\n",
    "    )\n",
    "\n",
    "    context = retrieve_context(query)\n",
    "    messages = [\n",
    "        {\n",
    "            \"content\": f\"Context: {context}\\nBased on the context above, answer the following question:\",\n",
    "            \"role\": \"system\",\n",
    "        },\n",
    "        {\"content\": query, \"role\": \"user\"},\n",
    "    ]\n",
    "    response = simple_chat(model_config=MODEL_CONFIG[\"nova_pro\"], messages=messages)\n",
    "\n",
    "    return {\"response\": response, \"statusCode\": 200}\n",
    "\n",
    "\n",
    "# User request\n",
    "print(rag_api(\"how you like the weather in Sydney? any comments?\"))\n",
    "\n",
    "langfuse_context.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the trace, you can see the retrieval function is called and the context is passed to the model as part of the system prompt. The fine model invocation takes both system prompt and user prompt and return the response.\n",
    "\n",
    "![langfuse-traces-use-case-3](./images/langfuse-trace-use-case-3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Case 4\n",
    "\n",
    "Multi-Modal Capabilities with Image Support\n",
    "\n",
    "Modern AI systems are increasingly adopting multi-modal capabilities, allowing them to process and understand different types of data inputs, including text, images, and audio. In this example, we demonstrate how Langfuse supports tracing for image-based inputs, which is particularly valuable for:\n",
    "\n",
    "1. **Image Analysis**: Interpreting and describing visual content\n",
    "2. **Visual Question Answering**: Answering questions based on image context\n",
    "3. **Document Processing**: Extracting information from scanned documents or images\n",
    "4. **Content Moderation**: Identifying inappropriate or sensitive visual content\n",
    "\n",
    "The implementation uses a structured message format where the image URL is passed as part of the user prompt, enabling the model to process both textual queries and visual information simultaneously. This capability is especially useful in applications like:\n",
    "\n",
    "- E-commerce product recognition\n",
    "- Medical image analysis\n",
    "- Social media content understanding\n",
    "\n",
    "Langfuse's tracing capabilities extend to these multi-modal interactions, providing visibility into how the model processes and responds to image inputs, which is crucial for debugging and improving these complex systems.\n",
    "\n",
    "In this use case, we will pass images as part of the user prompt and the model will process both the text and the image and langfuse will trace the entire process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'The image depicts a man dressed in formal attire, standing on a stage and appearing to give a presentation. His right hand is stretched out, suggesting that he is either gesturing to emphasize a point or clicking a button on a remote control. The backdrop features a large screen displaying the text \"Amazon Bedrock\". The tagline underneath the main text, \"The easiest way to build and scale generative AI applications\", suggests that he might be discussing this product or service during his presentation.', 'statusCode': 200}\n"
     ]
    }
   ],
   "source": [
    "@observe(name=\"Multi-Modal Image Example\")\n",
    "def vision_api(\n",
    "    query: str,\n",
    "    image_url: str,\n",
    ") -> Optional[str]:\n",
    "    langfuse_context.update_current_trace(\n",
    "        user_id=\"nova-user-1\",\n",
    "        session_id=\"vision-session\",\n",
    "        tags=[\"lab1\", \"vision-example\"],\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an AI trained to describe and interpret images.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": query},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"response\": simple_chat(\n",
    "            model_config=MODEL_CONFIG[\"nova_pro\"], messages=messages\n",
    "        ),\n",
    "        \"statusCode\": 200,\n",
    "    }\n",
    "\n",
    "\n",
    "# image source: https://www.aboutamazon.com/news/aws/aws-reinvent-2024-keynote-live-news-updates\n",
    "print(\n",
    "    vision_api(\n",
    "        query=\"What is happening in this image?\",\n",
    "        image_url=\"https://amazon-blogs-brightspot.s3.amazonaws.com/df/82/368cb270402e9739f04905ea9b19/swami-bedrock.jpeg\",\n",
    "    )\n",
    ")\n",
    "\n",
    "langfuse_context.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langfuse also supports tracing with image input, this is very useful for a multi-modal use case in which the model can take image as input.\n",
    "\n",
    "![langfuse-traces-use-case-4](./images/langfuse-trace-use-case-4.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool Use with Langfuse Tracing\n",
    "Tool use enables AI models to interact with external functions and APIs, extending their capabilities beyond pure text generation. This is particularly useful for:\n",
    "- Accessing real-time data (e.g., weather, stock prices)\n",
    "- Performing complex calculations\n",
    "- Integrating with external systems\n",
    "- Extracting structured data from unstructured sources (e.g. text, images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Case 1\n",
    "\n",
    "The example below demonstrates a weather information tool implementation. When a user asks about weather conditions, the model will:\n",
    "\n",
    "1. Recognize the need for weather data\n",
    "2. Extract location/unit parameters through structured tool definition\n",
    "3. Return a formatted response using our `get_current_weather` tool\n",
    "\n",
    "Expected result: The model should identify San Francisco as the location and celsius as the preferred unit, returning a structured tool response while maintaining full trace visibility in Langfuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': [{'index': 0, 'id': 'tooluse_1PeqgYYrTrWwZlb9vYHc4A', 'type': 'function', 'function': {'name': 'get_current_weather', 'arguments': '{\"unit\": \"celsius\", \"location\": \"San Francisco, CA\"}'}}], 'statusCode': 200}\n"
     ]
    }
   ],
   "source": [
    "@observe(name=\"Tool Use Example\")\n",
    "def tool_use_api(query: str) -> list:\n",
    "    langfuse_context.update_current_trace(\n",
    "        user_id=\"nova-user-1\",\n",
    "        session_id=\"tool-use-session\",\n",
    "        tags=[\"lab1\", \"tool-use\"],\n",
    "    )\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": query}]\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_current_weather\",\n",
    "                \"description\": \"Get the current weather in a given location\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"location\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                        },\n",
    "                        \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "                    },\n",
    "                    \"required\": [\"location\"],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"response\": converse_tool_use(\n",
    "            messages, tools, tool_choice=\"auto\", **MODEL_CONFIG[\"nova_pro\"]\n",
    "        ),\n",
    "        \"statusCode\": 200,\n",
    "    }\n",
    "\n",
    "\n",
    "print(tool_use_api(query=\"What's the weather like in San Francisco?, in celsius?\"))\n",
    "\n",
    "langfuse_context.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![langfuse-traces-tool-use](./images/langfuse-trace-tool-use.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Case 2\n",
    "The example below demonstrates a multi-modal document transcription tool implementation. When a user asks to transcribe a document, the model will:\n",
    "\n",
    "1. Recognize the schema for invoice transcription\n",
    "2. Extract structured data from images through structured tool definition\n",
    "3. Apply dependentSchemas to provide additional classification and reasoning for the extraction\n",
    "\n",
    "Expected result: The model should extract all the metadata and line items from the invoice and output the structured data in JSON format with classification and reasoning for each field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': [{'index': 0, 'id': 'tooluse_B3YuFLLGRVC5FNtdaXXirQ', 'type': 'function', 'function': {'name': 'transcribe_documents', 'arguments': '{\"documents\": [[{\"doc_type\": {\"value\": \"Receipt\", \"inference\": 0, \"source\": \"EXPLICIT\"}, \"receipt_number\": {\"value\": \"123PQR456\", \"inference\": 0, \"source\": \"EXPLICIT\"}, \"doc_amount_total\": {\"value\": 6231.09, \"inference\": 0, \"source\": \"EXPLICIT\"}, \"currency\": {\"value\": \"USD\", \"inference\": 0, \"source\": \"EXPLICIT\"}, \"vendor_business_number\": {\"value\": \"\", \"inference\": 2, \"source\": \"MISSING\"}, \"vendor_name\": {\"value\": \"Amazon.com\", \"inference\": 0, \"source\": \"EXPLICIT\"}, \"vendor_address\": {\"value\": \"123 Some Street, Some City, XYZ, USA\", \"inference\": 0, \"source\": \"EXPLICIT\"}, \"vendor_phone\": {\"value\": \"+1333-333-3333\", \"inference\": 0, \"source\": \"EXPLICIT\"}, \"vendor_email\": {\"value\": \"www.amazon.com\", \"inference\": 0, \"source\": \"EXPLICIT\"}, \"payment_method\": {\"value\": \"\", \"inference\": 2, \"source\": \"MISSING\"}, \"date_issued\": {\"value\": \"2021-01-01T00:00:00\", \"inference\": 0, \"source\": \"EXPLICIT\"}, \"line_items_amount_total\": {\"value\": 6169.4, \"inference\": 0, \"source\": \"EXPLICIT\"}, \"receiver_name\": {\"value\": \"XYZ Company\", \"inference\": 0, \"source\": \"EXPLICIT\"}, \"receiver_address\": {\"value\": \"789 Another Avenue, Different City, XYZ, USA\", \"inference\": 0, \"source\": \"EXPLICIT\"}, \"receiver_phone\": {\"value\": \"+1222-222-2222\", \"inference\": 0, \"source\": \"EXPLICIT\"}, \"receiver_email\": {\"value\": \"receiver@xyzcompany.com\", \"inference\": 0, \"source\": \"EXPLICIT\"}, \"payment_terms\": {\"value\": \"30 days\", \"inference\": 0, \"source\": \"EXPLICIT\"}, \"due_date\": {\"value\": \"2021-01-31T00:00:00\", \"inference\": 0, \"source\": \"EXPLICIT\"}, \"vat_id\": {\"value\": \"12-3456789\", \"inference\": 0, \"source\": \"EXPLICIT\"}}]]}'}}], 'statusCode': 200}\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"\"\"\n",
    "<instructions>\n",
    "  - Ensure to escape quotes in the JSON response\n",
    "  - Return \"\" for missing field values\n",
    "  - Apply dependentSchemas to all <document/> fields\n",
    "</instructions>\n",
    "\n",
    "<document>\n",
    "{\n",
    "    \"$schema\": \"https://json-schema.org/draft/2020-12/schema\",\n",
    "    \"$id\": \"/schemas/document\",\n",
    "    \"type\": \"object\",\n",
    "    \"description\": \"A document with the fields to transcribe\",\n",
    "    \"properties\": {\n",
    "        \"doc_type\": { \"properties\":{\"value\":{\"type\":\"string\"}}, \"description\": \"Type of Document: Receipt\" },\n",
    "        \"receipt_number\": { \"properties\":{\"value\":{\"type\":\"string\"}}, \"description\": \"The receipt number or other identifier number\" },\n",
    "        \"doc_amount_total\": { \"properties\":{\"value\":{\"type\":\"number\"}}, \"description\": \"The total receipt amount\" },\n",
    "        \"currency\": { \"properties\":{\"value\":{\"type\":\"string\"}}, \"description\": \"AUD/USD/CAD\" },\n",
    "        \"vendor_business_number\": { \"properties\":{\"value\":{\"type\":\"string\"}}, \"description\": \"Vendor's business identification number e.g. ABN\" },\n",
    "        \"vendor_name\": { \"properties\":{\"value\":{\"type\":\"string\"}}, \"description\": \"Business name issueing the receipt\" },\n",
    "        \"vendor_address\": { \"properties\":{\"value\":{\"type\":\"string\"}}, \"description\": \"Vendor's site address\" },\n",
    "        \"vendor_phone\": { \"properties\":{\"value\":{\"type\":\"string\"}}, \"description\": \"Vendor's phone number\" },\n",
    "        \"payment_method\": { \"properties\":{\"value\":{\"type\":\"string\"}}, \"description\": \"The payment type, e.g. EFTPOS, Card\" },\n",
    "        \"date_issued\": { \"properties\":{\"value\":{\"format\": \"YYYY-MM-DDThh:mm:ss\"}}, \"description\": \"Date document was issued\"},\n",
    "        \"line_items_amount_total\": { \"properties\":{\"value\":{\"type\":\"number\"}}, \"description\": \"Calculated sum of line item's line_amount fields\" }\n",
    "    },\n",
    "    \"dependentSchemas\": {\n",
    "        \"value\": {\n",
    "            \"properties\": {\n",
    "                \"inference\": { \"type\": \"integer\", \"description\": \"0=EXPLICIT|1=DERIVED|2=MISSING|3=OTHER\" },\n",
    "                \"source\": { \"type\": \"string\", \"description\": \"Source locations in the document for explicit and derived fields\" }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "<document/>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@observe(name=\"Vision Tool Use Example\")\n",
    "def vision_tool_use_api(\n",
    "    query: str,\n",
    "    image_url: str,\n",
    ") -> list:\n",
    "    langfuse_context.update_current_trace(\n",
    "        user_id=\"nova-user-1\",\n",
    "        session_id=\"tool-use-session\",\n",
    "        tags=[\"lab1\", \"tool-use\"],\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": query},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"transcribe_documents\",\n",
    "                \"description\": \"Extract all <document/> fields with the highest accuracy following <instructions/>\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"documents\": {\n",
    "                            \"type\": \"array\",\n",
    "                            \"items\": {\"$ref\": \"/schemas/document\"},\n",
    "                        },\n",
    "                    },\n",
    "                    \"required\": [\"documents\"],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"response\": converse_tool_use(\n",
    "            messages, tools, tool_choice=\"auto\", **MODEL_CONFIG[\"nova_pro\"]\n",
    "        ),\n",
    "        \"statusCode\": 200,\n",
    "    }\n",
    "\n",
    "\n",
    "# image source: https://aws.amazon.com/blogs/machine-learning/announcing-expanded-support-for-extracting-data-from-invoices-and-receipts-using-amazon-textract/\n",
    "print(\n",
    "    vision_tool_use_api(\n",
    "        query=\"Transcribe the invoice. Make sure to apply dependentSchemas to all <document/> fields\",\n",
    "        image_url=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2021/07/22/ml3911-img17.jpg\",\n",
    "    )\n",
    ")\n",
    "\n",
    "langfuse_context.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src=\"./images/langfuse-trace-tool-use-vision.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Management\n",
    "### What is prompt management?\n",
    "\n",
    "Prompt management is a systematic approach to storing, versioning and retrieving prompts in LLM applications. Key aspects of prompt management include version control, decoupling prompts from code, monitoring, logging and optimizing prompts as well as integrating prompts with the rest of your application and tool stack.\n",
    "\n",
    "Use Langfuse to effectively **manage** and **version** your prompts. Langfuse prompt management is a Prompt **CMS** (Content Management System).\n",
    "\n",
    "\n",
    "### Why use prompt management?\n",
    "\n",
    "Typical benefits of using a CMS apply here:\n",
    "\n",
    "- Decoupling: deploy new prompts without redeploying your application.\n",
    "- Non-technical users can create and update prompts via Langfuse Console.\n",
    "- Quickly rollback to a previous version of a prompt.\n",
    "- Compare different prompt versions side-by-side.\n",
    "\n",
    "Platform benefits:\n",
    "\n",
    "- Track performance of prompt versions in Langfuse Tracing.\n",
    "- Performance benefits compared to other implementations:\n",
    "\n",
    "-  No latency impact after first use of a prompt due to client-side caching and asynchronous cache refreshing.\n",
    "-  Support for text and chat prompts.\n",
    "-  Edit/manage via UI, SDKs, or API.\n",
    "\n",
    "\n",
    "There are several ways you can create prompts in Langfuse:\n",
    "\n",
    "-  Langfuse Console\n",
    "-  Langfuse SDK\n",
    "-  Langfuse API\n",
    "\n",
    "In this workshop, we will be using Langfuse Python low-level SDK to create prompts by reusing the prompt exampels from the Modul1 - Prompt Engineering with Amazon Bedrock and Nova Model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langfuse.model.ChatPromptClient at 0x7f46fce8e6b0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize Langfuse client\n",
    "langfuse = Langfuse()\n",
    "\n",
    "# Create a chat prompt without COT\n",
    "langfuse.create_prompt(\n",
    "    name=\"software-development-project-management-without-COT\",\n",
    "    type=\"chat\",\n",
    "    prompt=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"You are a project manager for a small software development team tasked with launching a new app feature. You want to streamline the development process and ensure timely delivery.\",\n",
    "        }\n",
    "    ],\n",
    "    labels=[\"dev\"],\n",
    "    config={\n",
    "        \"model\": MODEL_CONFIG[\"nova_pro\"][\"model_id\"],\n",
    "        \"maxTokens\": MODEL_CONFIG[\"nova_pro\"][\"inferenceConfig\"][\"maxTokens\"],\n",
    "        \"temperature\": MODEL_CONFIG[\"nova_pro\"][\"inferenceConfig\"][\"temperature\"],\n",
    "    },  # for Dev and experiment phase\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langfuse.model.ChatPromptClient at 0x7f47002950c0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a chat prompt with COT\n",
    "langfuse.create_prompt(\n",
    "    name=\"software-development-project-lead-with-COT\",\n",
    "    type=\"chat\",\n",
    "    prompt=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\"\"You are a project leader with technical expertise for a small software development team tasked with launching a new app feature. You want to streamline the development process and ensure timely delivery. Please follow these steps:\\n\n",
    "       {{step1}}\\n\n",
    "       \\n\n",
    "       {{step2}}\\n\n",
    "       \\n\n",
    "       {{step3}}\\n\n",
    "       \\n\n",
    "       {{step4}}\\n\"\"\",\n",
    "        }\n",
    "    ],\n",
    "    labels=[\"dev\"],\n",
    "    config={\n",
    "        \"model\": MODEL_CONFIG[\"nova_pro\"][\"model_id\"],\n",
    "        \"maxTokens\": MODEL_CONFIG[\"nova_pro\"][\"inferenceConfig\"][\"maxTokens\"],\n",
    "        \"temperature\": MODEL_CONFIG[\"nova_pro\"][\"inferenceConfig\"][\"temperature\"],\n",
    "    },  # for Dev and experiment phase\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the two langfuse prompts are created successfully.\n",
    "\n",
    "![langfuse-traces-prompt-management](./images/langfuse-prompt-management.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, fetch both prompts and fill in the values for the variables and call the prompts\n",
    "langfuse = Langfuse()\n",
    "\n",
    "# Get current latest version of a prompt\n",
    "sdpm_with_cot_prompt = langfuse.get_prompt(\n",
    "    \"software-development-project-management-with-COT\", type=\"chat\", label=\"dev\"\n",
    ")\n",
    "# Insert variables into prompt template\n",
    "sdpm_with_cot_prompt_compiled = sdpm_with_cot_prompt.compile(\n",
    "    step1=\"Define Requirements\",\n",
    "    step2=\"Breakdown into Tasks\",\n",
    "    step3=\"Set Deadlines\",\n",
    "    step4=\"Monitor Progress and Optimize\",\n",
    ")\n",
    "\n",
    "sdpm_without_cot_prompt = langfuse.get_prompt(\n",
    "    \"software-development-project-management-without-COT\", type=\"chat\", label=\"dev\"\n",
    ")\n",
    "sdpm_without_cot_prompt_compiled = sdpm_without_cot_prompt.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'You are a project manager for a small software development team tasked with launching a new app feature. You want to streamline the development process and ensure timely delivery. Please follow these steps:\\n\\n       Define Requirements\\n\\n       \\n\\n       Breakdown into Tasks\\n\\n       \\n\\n       Set Deadlines\\n\\n       \\n\\n       Monitor Progress and Optimize\\n',\n",
       "  'role': 'user'}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdpm_with_cot_prompt_compiled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can add the prompt object to the generation call in the SDKs to link the generation in Langfuse Tracing to the prompt version. This linkage enables tracking of metrics by prompt version and name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a project manager for a small software development team tasked with launching a new app feature. You want to streamline the development process and ensure timely delivery. Please follow these steps:\\n\\n       Define Requirements\\n\\n       \\n\\n       Breakdown into Tasks\\n\\n       \\n\\n       Set Deadlines\\n\\n       \\n\\n       Monitor Progress and Optimize\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdpm_with_cot_prompt_compiled[0][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converesation according to AWS spec including prompting + history\n",
    "@observe()\n",
    "def main():\n",
    "    langfuse_context.update_current_trace(\n",
    "        name=\"prompt-management-trace\",\n",
    "        user_id=\"nova-user-1\",\n",
    "        session_id=\"link-prompt-session\",\n",
    "        tags=[\"lab1\"],\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": sdpm_with_cot_prompt_compiled[0][\"role\"],\n",
    "            \"content\": sdpm_with_cot_prompt_compiled[0][\"content\"],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    simple_chat(\n",
    "        model_config=MODEL_CONFIG[\"nova_pro\"],\n",
    "        messages=messages,\n",
    "        prompt=sdpm_with_cot_prompt,\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": sdpm_without_cot_prompt_compiled[0][\"role\"],\n",
    "            \"content\": sdpm_without_cot_prompt_compiled[0][\"content\"],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    simple_chat(\n",
    "        model_config=MODEL_CONFIG[\"nova_pro\"],\n",
    "        messages=messages,\n",
    "        prompt=sdpm_without_cot_prompt,\n",
    "    )\n",
    "\n",
    "\n",
    "main()\n",
    "langfuse_context.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the trace is linked to the prompt version and the prompt name. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![langfuse-traces-prompt-management](./images/langfuse-link-prompt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab1 Summary:\n",
    "In Lab1, we explored the basics of integrating Langfuse with AWS to manage prompt traces effectively.\n",
    "We demonstrated how to update traces, link prompts to user sessions, and visualize these linkages with a practical example.\n",
    "\n",
    "As we conclude this lab, take a moment to reflect on the foundational skills you've gained.\n",
    "Now, if you are at an AWS event, you can return to the workshop studio for additional instructions before moving into the next lab, where we will dive deeper into RAG related tracing and evaluation.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
