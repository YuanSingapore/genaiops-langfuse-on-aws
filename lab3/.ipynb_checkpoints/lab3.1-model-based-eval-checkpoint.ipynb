{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Lab 3.1: Evaluate Langfuse Traces using an external evaluation pipeline\n",
    "\n",
    "#### An external evaluation pipeline is useful when you need:\n",
    "- More control over when traces get evaluated. You could schedule the pipeline to run at specific times or responding to event-based triggers like Webhooks.\n",
    "- Greater flexibility with your custom evaluations, when your needs go beyond what’s possible with the Langfuse UI\n",
    "- Version control for your custom evaluations\n",
    "- The ability to evaluate data using existing evaluation frameworks and pre-defined metrics\n",
    "\n",
    "In this notebook, we will learn to implement a external evaluation pipeline by doing the following:\n",
    "1. Create a synthetic dataset to test your models.\n",
    "2. Use the Langfuse client to gather and filter traces of previous model runs\n",
    "3. Evaluate these traces offline and incrementally\n",
    "4. Add scores to existing Langfuse traces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "> ℹ️ You can **skip these prerequisite steps** if you're in an instructor-led workshop using temporary accounts provided by AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following line to install dependencies if you are not using AWS workshop environment\n",
    "# %pip install langfuse datasets ragas python-dotenv langchain-aws boto3 --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Connect to self-hosted or cloud Langfuse environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you already define the environment variables in the .env of the vscode server, please skip the following cell\n",
    "# Define the environment variables for langfuse\n",
    "# You can find those values when you create the API key in Langfuse\n",
    "# import os\n",
    "# os.environ[\"LANGFUSE_SECRET_KEY\"] = \"xxxx\" # Your Langfuse project secret key\n",
    "# os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"xxxx\" # Your Langfuse project public key\n",
    "# os.environ[\"LANGFUSE_HOST\"] = \"xxx\" # Langfuse domain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Initialization and Authentication Check\n",
    "Run the following cells to initialize common libraries and clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import boto3\n",
    "from langfuse.decorators import langfuse_context, observe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "Initialize AWS Bedrock clients and check models available in your account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to access Bedrock configuration\n",
    "bedrock = boto3.client(service_name=\"bedrock\", region_name=\"us-west-2\")\n",
    "\n",
    "# Check if Nova models are available in this region\n",
    "models = bedrock.list_inference_profiles()\n",
    "nova_found = False\n",
    "for model in models[\"inferenceProfileSummaries\"]:\n",
    "    if (\n",
    "        \"Nova Pro\" in model[\"inferenceProfileName\"]\n",
    "        or \"Nova Lite\" in model[\"inferenceProfileName\"]\n",
    "        or \"Nova Micro\" in model[\"inferenceProfileName\"]\n",
    "    ):\n",
    "        print(\n",
    "            f\"Found Nova model: {model['inferenceProfileName']} - {model['inferenceProfileId']}\"\n",
    "        )\n",
    "        nova_found = True\n",
    "if not nova_found:\n",
    "    raise ValueError(\n",
    "        \"No Nova models found in available models. Please ensure you have access to Nova models.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "Initialize the Langfuse client and check credentials are valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse import Langfuse\n",
    "\n",
    "# langfuse client\n",
    "langfuse = Langfuse()\n",
    "if langfuse.auth_check():\n",
    "    print(\"Langfuse has been set up correctly\")\n",
    "    print(f\"You can access your Langfuse instance at: {os.environ['LANGFUSE_HOST']}\")\n",
    "else:\n",
    "    print(\n",
    "        \"Credentials not found or invalid. Check your Langfuse API key and host in the .env file.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Langfuse Wrappers for Bedrock Converse API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath('..'))  # Add parent directory to path\n",
    "from config import MODEL_CONFIG\n",
    "from utils import converse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "# Generate synthetic data\n",
    "\n",
    "In this notebook, we consider a use case of leveraging a LLM to generate product descriptions that can be used in advising the product on an e-commerce page. The first step is to generate a list of products and for each of them, instruct Amazon Nova Lite to \n",
    "generate brief product descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's prompt the model to generate 50 products\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"For a variety of 50 different product categories sold on a e-commerce website, \\\n",
    "    generate one product that is interesting to a consumer. The product names should be reflective of the actual product being sold. \\\n",
    "    Generate the 50 product items as comma separated values. Do not generate any additional words apart from the product names\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Make the API call to the Nova Lite model\n",
    "model_response = converse(messages=messages, **MODEL_CONFIG[\"nova_lite\"])\n",
    "\n",
    "# Print the generated text\n",
    "print(\"\\n[Response Content Text]\")\n",
    "print(model_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the model generation outputs\n",
    "products = [item.strip() for item in model_response.split(\",\")]\n",
    "\n",
    "for prd in products:\n",
    "    print(prd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "For each of the products, we will now generate product descriptions using Amazon Nova Lite, and capture the traces to Langfuse using the ```@observe()``` decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate product descriptions for each product\n",
    "prompt_template = \"You are a product marketer and you need to generate detailed \\\n",
    "product descriptions for products which will be used for selling \\\n",
    "the product on a e-commerce website. Any catchy phrases from the \\\n",
    "descriptions will also be used for social meda campaigns. \\\n",
    "From the product descriptions, customers should be able to understand \\\n",
    "how the product can help them in their lives but also be able to trust \\\n",
    "this company. Your descriptions are fun and engaging. \\\n",
    "Your answer should be 4 sentences at max.\"\n",
    "\n",
    "\n",
    "@observe(name=\"Batch Product Description Generation\")\n",
    "def main():\n",
    "    langfuse_context.update_current_trace(\n",
    "        user_id=\"nova-user-1\",\n",
    "        session_id=\"nova-batch-generation-session\",\n",
    "        tags=[\"lab3.1\"],\n",
    "    )\n",
    "\n",
    "    for product in products:\n",
    "        print(f\"Input: Generate a description for {product}\")\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": prompt_template},\n",
    "            {\"role\": \"user\", \"content\": f\"Generate a description for {product}\"},\n",
    "        ]\n",
    "        response = converse(\n",
    "            messages, metadata={\"product\": product}, **MODEL_CONFIG[\"nova_lite\"]\n",
    "        )\n",
    "        print(f\"Output: {response} \\n\")\n",
    "\n",
    "\n",
    "main()\n",
    "\n",
    "langfuse_context.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "Now you should see these product descriptions in the Traces section of the langfuse UI.\n",
    "\n",
    "![Traces collected from the LLM generations](./images/product_description_traces.png \"Langfuse Traces\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "The goal of this tutorial is to show you how to build an model-based evaluation pipeline. These pipelines will run in your CI/CD environment, or be run in a different orchestrated container service. No matter the environment you choose, three key steps always apply:\n",
    "\n",
    "1. Fetch Your Traces: Get your application traces to your evaluation environment\n",
    "2. Run Your Evaluations: Apply any evaluation logic you prefer\n",
    "3. Save Your Results: Attach your evaluations back to the Langfuse trace used for calculating them.\n",
    "\n",
    "***\n",
    "Goal: This evaluation pipeline is executed on all the traces over the past 24 hours\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## 1. Fetch the traces\n",
    "\n",
    "The ```fetch_traces()``` function has arguments to filter the traces by tags, timestamps, and beyond. We can also choose the number of samples for pagination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "now = datetime.now()\n",
    "last_24_hours = now - timedelta(days=1)\n",
    "\n",
    "traces_batch = langfuse.fetch_traces(\n",
    "    page=1,\n",
    "    limit=1,\n",
    "    tags=\"lab3.1\",\n",
    "    session_id=\"nova-batch-generation-session\",\n",
    "    from_timestamp=last_24_hours,\n",
    "    to_timestamp=datetime.now(),\n",
    ").data\n",
    "\n",
    "print(f\"Trace ID: {traces_batch[0].id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = langfuse.fetch_observations(trace_id=traces_batch[0].id, type=\"GENERATION\").data\n",
    "observations[0].output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## 2. Categorical Evaluation using LLM-as-a-judge\n",
    "\n",
    "Evaluation functions should take a trace as input and yield a valid score.\n",
    "When analyzing the outputs of your LLM applications, you may want to evaluate traits that are defined qualitatively such as readability, helpfulness or measures for reducing hallucinations such as completeness.\n",
    "\n",
    "We're building product descriptions and to ensure it resonates with customers, we want to measure readability. For more LLM-as-a-judge definitions, check out the judge based evaluator prompts defined in the [Amazon Bedrock Evaluator Prompts](https://docs.aws.amazon.com/bedrock/latest/userguide/model-evaluation-type-judge-prompt.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_readability = \"\"\"\n",
    "You are a helpful agent that can assess an LLM response according to the given rubrics.\n",
    "\n",
    "You are given a product description generated by a LLM. Your task is to assess the readability of the LLM response to the question, in other words, how easy it is for a typical reading audience to comprehend the response at a normal reading rate.\n",
    "\n",
    "Please rate the readability of the response based on the following scale:\n",
    "- unreadable: The response contains gibberish or could not be comprehended by any normal audience.\n",
    "- poor readability: The response is comprehensible, but it is full of poor readability factors that make comprehension very challenging.\n",
    "- fair readability: The response is comprehensible, but there is a mix of poor readability and good readability factors, so the average reader would need to spend some time processing the text in order to understand it.\n",
    "- good readability: Very few poor readability factors. Mostly clear, well-structured sentences. Standard vocabulary with clear context for any challenging words. Clear organization with topic sentences and supporting details. The average reader could comprehend by reading through quickly one time.\n",
    "- excellent readability: No poor readability factors. Consistently clear, concise, and varied sentence structures. Simple, widely understood vocabulary. Logical organization with smooth transitions between ideas. The average reader may be able to skim the text and understand all necessary points.\n",
    "\n",
    "Here is the product description that needs to be evaluated: {prd_desc}\n",
    "\n",
    "Firstly explain your response, followed by your final answer. You should follow the format\n",
    "Explanation: [Explanation], Answer: [Answer],\n",
    "where '[Answer]' can be one of the following:\n",
    "```\n",
    "unreadable\n",
    "poor readability\n",
    "fair readability\n",
    "good readability\n",
    "excellent readability\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "@observe(name=\"Readability Evaluation\")\n",
    "def generate_readability_score(trace_output):\n",
    "    prd_desc_readability = template_readability.format(prd_desc=trace_output)\n",
    "    # query = [f\"Rate the readability of product description: {traces_batch[1].output}\"]\n",
    "    readability_score = converse(\n",
    "            messages=[{\"role\": \"user\", \"content\": prd_desc_readability}], **MODEL_CONFIG[\"nova_pro\"]\n",
    "        )\n",
    "    explanation, score = readability_score.split(\"\\n\\n\")\n",
    "    return explanation, score\n",
    "\n",
    "\n",
    "print(f\"User query: {observations[0].input[1]['content']}\")\n",
    "print(f\"Model answer: {observations[0].output}\")\n",
    "explanation, score = generate_readability_score(observations[0].output)\n",
    "print(f\"Readability: {score}, Explanation: {explanation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## 3. Add the evaluation to the trace\n",
    "\n",
    "Now that we have generated a readability score as well as a explanation, we can use the Langfuse client to add scores to existing traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "langfuse.score(\n",
    "    trace_id=traces_batch[0].id,\n",
    "    observation_id=observations[0].id,\n",
    "    name=\"readability\",\n",
    "    value=score,\n",
    "    comment=explanation,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "# Putting everything together\n",
    "\n",
    "We just saw how to do this for one trace, let's put it all together in a function to run it on all the traces collected in the last 24 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "@observe(name=\"Batch Readability Evaluation\")\n",
    "def batch_evaluate():\n",
    "    langfuse_context.update_current_trace(\n",
    "        user_id=\"nova-user-1\",\n",
    "        session_id=\"nova-batch-evals-session\",\n",
    "        tags=[\"lab3.1\"],\n",
    "    )\n",
    "    traces_batch = langfuse.fetch_traces(\n",
    "        page=1,\n",
    "        limit=1,\n",
    "        tags=\"lab3.1\",\n",
    "        session_id=\"nova-batch-generation-session\",\n",
    "        from_timestamp=last_24_hours,\n",
    "        to_timestamp=datetime.now(),\n",
    "    ).data\n",
    "\n",
    "    observations = langfuse.fetch_observations(trace_id=traces_batch[0].id, type=\"GENERATION\").data\n",
    "\n",
    "    for observation in observations[-5:]: # Only evaluate the last 5 observations to save time\n",
    "        print(f\"Processing {observation.id}\")\n",
    "        if observation.output is None:\n",
    "            print(\n",
    "                f\"Warning: \\n Trace {observation.id} had no generated output, \\\n",
    "            it was skipped\"\n",
    "            )\n",
    "            continue\n",
    "        explanation, score = generate_readability_score(observation.output)\n",
    "        langfuse.score(\n",
    "            trace_id=traces_batch[0].id,\n",
    "            observation_id=observation.id,\n",
    "            name=\"readability\",\n",
    "            value=score,\n",
    "            comment=explanation,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_evaluate()\n",
    "\n",
    "langfuse_context.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "#### If your pipeline ran successfully, you should now see scores added to your traces\n",
    "\n",
    "![Langfuse Trace with score added for readability](./images/scored_trace.png \"Scored trace on langfuse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### Congratuations\n",
    "You have successfully finished Lab 3.1.\n",
    "\n",
    "If you are at an AWS event, you can return to the workshop studio for additional instructions before moving into the next lab, where we will explore GenAI guardrails."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
