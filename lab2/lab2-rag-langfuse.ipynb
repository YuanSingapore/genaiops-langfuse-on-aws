{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Retrieval-Augmented Generation (RAG) pipelines with Ragas and Langfuse\n",
    "\n",
    "In this notebook we'll explore ways to evaluate the quality of Retrieval-Augmented Generation (RAG) pipelines with the opensource tools like [RAGAS](https://docs.ragas.io/en/v0.1.21/index.html) and leverage the features in [Langfuse](https://langfuse.com/) to manage and trace the RAG pipelines with traces and spans. We will create a Bedrock knowledge base and the RAG batch generation results to show offline evaluation and scoring.\n",
    "\n",
    "> ℹ️ Note: This notebook requires user configurations for some steps. \n",
    ">\n",
    "> When a cell requires user configurations, you will see a message like this callout with the 👉 emoji.\n",
    ">\n",
    "> Pay attention to the instructions with the 👉 emoji and perform the configurations in the AWS Console or in the corresponding cell before running the code cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "\n",
    "> If you haven't selected the kernel, please click on the \"Select Kernel\" button at the upper right corner, select Python Environments and choose \".venv (Python 3.9.20) .venv/bin/python Recommended\".\n",
    "\n",
    "> To execute each notebook cell, press Shift + Enter.\n",
    "\n",
    "> ℹ️ You can **skip these prerequisite steps** if you're in an instructor-led workshop using temporary accounts provided by AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional permissions for Amazon OpenSearch\n",
    "\n",
    "To complete the manual Bedrock Knowledge setup steps in this notebook, your **AWS Console user/role** will need:\n",
    "\n",
    "- [Permissions to work with Amazon OpenSearch vector collections](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless-vector-search.html)\n",
    "- Permission to **create IAM roles** and attach policies to them, including: `iam:AttachRolePolicy`, `iam:CreateRole`, `iam:DetachRolePolicy`, `iam:GetRole`, `iam:PassRole`, `iam:CreatePolicy`, `iam:CreatePolicyVersion`, and `iam:DeletePolicyVersion`.\n",
    "\n",
    "> ℹ️ **Note:** In testing, we saw `NetworkError` issues when attempting to create Bedrock KBs using only the above-linked `aoss` policy statements. This was resolved by granting `aoss:*` on `*` instead, but you should consider reducing these permissions before using in production environments.\n",
    "\n",
    "Refer to the [AWS Console for Identity and Access Management (IAM)](https://console.aws.amazon.com/iam/home?#/home) to grant permissions to your user or role."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies and Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q langfuse==2.59.7 ragas python-dotenv langchain-aws boto3 --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets==2.10.1 in /opt/conda/lib/python3.10/site-packages (2.10.1)\n",
      "Requirement already satisfied: pyarrow==12.0.1 in /opt/conda/lib/python3.10/site-packages (12.0.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets==2.10.1) (1.26.4)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.10.1) (0.3.6)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.10.1) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.10.1) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.10.1) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.10.1) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.10.1) (0.70.14)\n",
      "Requirement already satisfied: fsspec>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.11.1->datasets==2.10.1) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.10.1) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.10.1) (0.23.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets==2.10.1) (23.2)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets==2.10.1) (0.18.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.10.1) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.10.1) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.10.1) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.10.1) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.10.1) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.10.1) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.10.1) (4.0.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets==2.10.1) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets==2.10.1) (4.14.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.10.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.10.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.10.1) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.10.1) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.10.1) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.10.1) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.10.1) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.10.1) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Complete setup for script-based datasets\n",
    "%pip install datasets==2.10.1  pyarrow==12.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please make sure you have completed the prerequisites to setup the Langfuse project and API keys in the .env file to connect to self-hosted or cloud Langfuse environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets imported successfully\n",
      "datasets location: /opt/conda/lib/python3.10/site-packages/datasets/__init__.py\n",
      "datasets version: 2.10.1\n"
     ]
    }
   ],
   "source": [
    "# check for dataset versions \n",
    "try:\n",
    "    import datasets\n",
    "    print(\"datasets imported successfully\")\n",
    "    print(f\"datasets location: {datasets.__file__}\")\n",
    "    # Try alternative version check\n",
    "    try:\n",
    "        print(f\"datasets version: {datasets.__version__}\")\n",
    "    except:\n",
    "        print(\"No __version__ attribute found\")\n",
    "        # Alternative version check\n",
    "        try:\n",
    "            from datasets.utils.version import __version__\n",
    "            print(f\"datasets version (alternative): {__version__}\")\n",
    "        except:\n",
    "            print(\"Could not determine datasets version\")\n",
    "except ImportError as e:\n",
    "    print(f\"Failed to import datasets: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you already define the environment variables in the .env of the vscode server, please skip the following cell\n",
    "# Define the environment variables for langfuse\n",
    "# You can find those values when you create the API key in Langfuse\n",
    "# Get keys for your project from the LangFuse settings page: https://cloud.langfuse.com\n",
    "import os, sys\n",
    "sys.path.append(os.path.abspath('..'))  # Add parent directory to path\n",
    "from utils import *\n",
    "secret=get_secret()\n",
    "secrets=json.loads(secret)\n",
    "os.environ['LANGFUSE_PUBLIC_KEY'] = secrets[\"LANGFUSE_PUBLIC_KEY\"]\n",
    "os.environ['LANGFUSE_SECRET_KEY'] = secrets[\"LANGFUSE_SECRET_KEY\"]\n",
    "os.environ['LANGFUSE_HOST'] = secrets['LANGFUSE_HOST'] #, 'https://cloud.langfuse.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [Langfuse documentation](https://langfuse.com/docs/get-started) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization and Authentication Check\n",
    "Run the following cells to initialize common libraries and clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Any, List, Optional\n",
    "\n",
    "# External Dependencies:\n",
    "import pandas as pd  # For working with tabular data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize AWS Bedrock clients and check models available in your account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US Anthropic Claude 3 Sonnet - us.anthropic.claude-3-sonnet-20240229-v1:0\n",
      "US Anthropic Claude 3 Opus - us.anthropic.claude-3-opus-20240229-v1:0\n",
      "US Anthropic Claude 3 Haiku - us.anthropic.claude-3-haiku-20240307-v1:0\n",
      "US Meta Llama 3.2 11B Instruct - us.meta.llama3-2-11b-instruct-v1:0\n",
      "US Meta Llama 3.2 3B Instruct - us.meta.llama3-2-3b-instruct-v1:0\n",
      "US Meta Llama 3.2 90B Instruct - us.meta.llama3-2-90b-instruct-v1:0\n",
      "US Meta Llama 3.2 1B Instruct - us.meta.llama3-2-1b-instruct-v1:0\n",
      "US Anthropic Claude 3.5 Sonnet - us.anthropic.claude-3-5-sonnet-20240620-v1:0\n",
      "US Anthropic Claude 3.5 Haiku - us.anthropic.claude-3-5-haiku-20241022-v1:0\n",
      "US Meta Llama 3.1 8B Instruct - us.meta.llama3-1-8b-instruct-v1:0\n",
      "US Meta Llama 3.1 70B Instruct - us.meta.llama3-1-70b-instruct-v1:0\n",
      "US Meta Llama 3.3 70B Instruct - us.meta.llama3-3-70b-instruct-v1:0\n",
      "US Anthropic Claude 3.5 Sonnet v2 - us.anthropic.claude-3-5-sonnet-20241022-v2:0\n",
      "US DeepSeek-R1 - us.deepseek.r1-v1:0\n",
      "US Mistral Pixtral Large 25.02 - us.mistral.pixtral-large-2502-v1:0\n",
      "US Llama 4 Scout 17B Instruct - us.meta.llama4-scout-17b-instruct-v1:0\n",
      "US Llama 4 Maverick 17B Instruct - us.meta.llama4-maverick-17b-instruct-v1:0\n",
      "US Nova Premier - us.amazon.nova-premier-v1:0\n",
      "US Claude Opus 4 - us.anthropic.claude-opus-4-20250514-v1:0\n",
      "US Anthropic Claude 3.7 Sonnet - us.anthropic.claude-3-7-sonnet-20250219-v1:0\n",
      "US Nova Pro - us.amazon.nova-pro-v1:0\n",
      "US Claude Sonnet 4 - us.anthropic.claude-sonnet-4-20250514-v1:0\n",
      "US Claude Opus 4.1 - us.anthropic.claude-opus-4-1-20250805-v1:0\n"
     ]
    }
   ],
   "source": [
    "import boto3  # General Python SDK for AWS (including Bedrock)\n",
    "\n",
    "# used to access Bedrock configuration\n",
    "bedrock = boto3.client(service_name=\"bedrock\", region_name=\"us-east-1\")\n",
    "\n",
    "bedrock_agent_runtime = boto3.client(\n",
    "    service_name=\"bedrock-agent-runtime\", region_name=\"us-east-1\"\n",
    ")\n",
    "\n",
    "# Check which models are available in your account\n",
    "models = bedrock.list_inference_profiles()\n",
    "for model in models[\"inferenceProfileSummaries\"]:\n",
    "    print(model[\"inferenceProfileName\"] + \" - \" + model[\"inferenceProfileId\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the Langfuse client and check credentials are valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langfuse has been set up correctly\n",
      "You can access your Langfuse instance at: http://langfu-loadb-w7udau70n77l-1274730217.ap-southeast-1.elb.amazonaws.com\n"
     ]
    }
   ],
   "source": [
    "from langfuse import Langfuse\n",
    "\n",
    "# langfuse client\n",
    "langfuse = Langfuse()\n",
    "if langfuse.auth_check():\n",
    "    print(\"Langfuse has been set up correctly\")\n",
    "    print(f\"You can access your Langfuse instance at: {os.environ['LANGFUSE_HOST']}\")\n",
    "else:\n",
    "    print(\n",
    "        \"Credentials not found or invalid. Check your Langfuse API key and host in the .env file.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the Knowledge Base\n",
    "Next, let's upload the documents to Amazon S3 and create a vector store (knowledge base) so we can perform retrieval-augmented generation (RAG) given a user query. In the following steps, we'll configure:\n",
    "\n",
    "- An Amazon S3 bucket_name to store our document corpus. \n",
    "- A folder prefix under the bucket where artifacts will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client(service_name=\"s3\", region_name=\"us-east-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "s3.delete_bucket(Bucket=\"langfuse-test-us-east-1\")\n",
    "s3.create_bucket(Bucket=\"langfuse-test-us-east-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket eval-369776982489-us-east-1 exists\n"
     ]
    }
   ],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "\n",
    "botosess = boto3.Session(region_name=\"us-east-1\")\n",
    "region = botosess.region_name\n",
    "account_id = boto3.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "bucket_name = f\"eval-{account_id}-{region}\"\n",
    "s3_prefix = \"bedrock-rag-eval\"\n",
    "\n",
    "# check if s3 bucket exists or not, if not, create bucket\n",
    "s3 = boto3.client(\"s3\")\n",
    "try:\n",
    "    s3.head_bucket(Bucket=bucket_name)\n",
    "    print(f\"Bucket {bucket_name} exists\")\n",
    "except ClientError:\n",
    "    print(f\"Creating bucket {bucket_name}\")\n",
    "    s3.create_bucket(\n",
    "        Bucket=bucket_name, CreateBucketConfiguration={\"LocationConstraint\": region}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the documents to Amazon S3\n",
    "\n",
    "First, we'll need to upload the sample documents to Amazon S3 - for which you can just run the code cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syncing corpus to:\n",
      "s3://eval-369776982489-us-east-1/bedrock-rag-eval/corpus/\n"
     ]
    }
   ],
   "source": [
    "corpus_s3uri = f\"s3://{bucket_name}/{s3_prefix}/corpus\"\n",
    "print(f\"Syncing corpus to:\\n{corpus_s3uri}/\")\n",
    "\n",
    "# We will use the AWS CLI to recursively sync the folder to the S3 bucket.\n",
    "!aws s3 sync --quiet ./datasets/corpus {corpus_s3uri}/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the knowledge base in AWS Console\n",
    "> 👉 This section includes steps you'll need to take manually, not just running the code cells!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to set up the actual Bedrock Knowledge Base for testing is **manually through the AWS Console**:\n",
    "\n",
    "1. First, **open** the [AWS Console for Amazon Bedrock](https://console.aws.amazon.com/bedrock/home?#/knowledge-bases) and select *Orchestration > Knowledge bases* from the left sidebar menu, as shown in the screenshot below:\n",
    "\n",
    "    > ℹ️ **Check** you're working in the correct *AWS Region* in the top right corner of the UI\n",
    "\n",
    "![KB Console](images/bedrock-kbs/01-bedrock-kb-console.png \"Screenshot of AWS Console for Amazon Bedrock Knowledge Bases, showing 'Create knowledge base' action button\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Click the **Create knowledge base** button and select **Knowledge Base with vector store**. In the screen that opens:\n",
    "\n",
    "- For **knowledge base name**, enter `example-squad-kb`\n",
    "- For **knowledge base description**, you can provide (something like) `Demo knowledge base for question answering evaluation`\n",
    "- Leave the other settings as default (allow creating a new execution role, and no tags)\n",
    "- Please chose Amazon S3 as the data source (default)\n",
    "\n",
    "Your configuration should look like the screenshot below:\n",
    "\n",
    "![KB Basics](images/bedrock-kbs/02a-create-kb-basics.png \"Screenshot of step 1 in Bedrock Knowledge Base creation workflow: with KB name, description, (create new) execution role, and (empty) tags configured. At the end of the form, a 'Next' button is visible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. In the **Next** screen, you'll configure the S3 data source.\n",
    "\n",
    "    Leave the data source as S3 and then select the bucket and prefix per you created in the previous step and use Amazon Bedrock default parser.\n",
    "\n",
    "![](images/bedrock-kbs/02b-create-kb-data-source.png \"Screenshot of Knowledge Base vector index settings including Cohere Embed Multilingual embedding model, and quick-create vector store. 'Next' button is visible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. In the **Next** screen, you'll configure the vector index:\n",
    "\n",
    "    For *embeddings model*, select `Cohere Embed Multilingual`\n",
    "\n",
    "    > ℹ️ **Check** in the [Amazon Bedrock Model Access console](https://console.aws.amazon.com/bedrock/home?#/modelaccess) that you've enabled access to this model in the current region.\n",
    "    >\n",
    "    > If needed, you should be able to select an alternative embedding model instead.\n",
    "\n",
    "    For *Vector database*, select `Quick create a new vector store`\n",
    "\n",
    "    You can find more information from this screen or the [Amazon Bedrock Developer Guide](https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base-setup.html) about the different vector stores Bedrock Knowledge Bases support. This default option will create a new [Amazon OpenSearch Serverless](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless-overview.html) cluster\n",
    "\n",
    "    Leave other settings at their defaults as shown below, and you should be ready to proceed:\n",
    "\n",
    "![](images/bedrock-kbs/02c-create-kb-index.png \"Screenshot of Knowledge Base vector index settings including Cohere Embed Multilingual embedding model, and quick-create vector store. 'Next' button is visible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Click **Next** to review your configuration, and then **Create knowledge base** to complete the process.\n",
    "\n",
    "    > ⏰ It might take **a few minutes** for the creation to complete. A progress indicator banner should be visible if you scroll up. Alternatively in a separate tab, you could check the [Amazon OpenSearch Serverless Collections console](https://console.aws.amazon.com/aos/home?#opensearch/collections) - where you should see the underlying vector collection being created.\n",
    "\n",
    "    Once your Knowledge Base is completed successfully, you'll be directed to the its detail screen as shown below:\n",
    "\n",
    "![](images/bedrock-kbs/03-kb-detail-page.png \"Detail screen for the created Amazon Bedrock Knowledge Base, showing creation success banner. Includes sections 'Knowledge base overview' (containing the KB ID, name, and other details); 'Tags' (empty); 'Data source' (one Amazon S3 data source listed); 'Embeddings model' (Cohere Embed); and an interactive 'Test knowledge base' chat sidebar on the right with a warning that some data sources have not been synced.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. As mentioned in the alert box shown ahead, your new knowledge base will not yet contain your documents until we **sync** the data source:\n",
    "\n",
    "    **Select** your S3 data source by selecting the checkbox to the left of it's name in the data sources list, and click the **Sync** button above to start the sync.\n",
    "\n",
    "    The *Status* will change to `Syncing` for a few seconds, after which it will return to `Available`\n",
    "\n",
    "![](images/bedrock-kbs/04a-kb-data-source-after-sync.png \"Screenshot of KB 'data source' section after running sync, with the data source selected and status showing as 'available'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the sync completed, your Knowledge Base should be ready to use.\n",
    "\n",
    "Optionally, you can click into your data source to check the sync `Added` the 20 files as expected:\n",
    "\n",
    "<img src=\"images/bedrock-kbs/04b-kb-data-sync-details.png\" width=\"600\" alt=\"Data source details screen showing sync completed successfully with 20 files detected and added to the index, and 0 files failed\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test out the Knowledge Base\n",
    "\n",
    "Before we discuss evaluation at scale, let's run a test queries to check the KB is working properly. Let's go back to the detail page of the knowledge base."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, you can find the knowledge base id is `Z746ERZP5X` in the screenshot below (please check your own *Knowledge Base ID*) on the top of the page in the *Knowledge Base overview* panel.\n",
    "\n",
    "![](images/bedrock-kbs/04c-kb-main-page.png \"Screenshot of the main page of the knowledge base\")\n",
    "\n",
    "👉 **Replace** the below placeholder with your knowledge base's unique ID, and run the cells below to continue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_base_id = \"UIKGHJDXAJ\"  # Something like \"Z746ERZP5X\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the ID identified, you can use the Bedrock runtime [RetrieveAndGenerate API](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_agent-runtime_RetrieveAndGenerate.html) to query your knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What kind of economy does Victoria have?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'account_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Use the RetrieveAndGenerate API with Nova Pro model to query the knowledge base\u001b[39;00m\n\u001b[1;32m      2\u001b[0m rag_resp \u001b[38;5;241m=\u001b[39m bedrock_agent_runtime\u001b[38;5;241m.\u001b[39mretrieve_and_generate(\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: query},\n\u001b[1;32m      4\u001b[0m     retrieveAndGenerateConfiguration\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mknowledgeBaseConfiguration\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m      6\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mknowledgeBaseId\u001b[39m\u001b[38;5;124m\"\u001b[39m: knowledge_base_id,\n\u001b[0;32m----> 7\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodelArn\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marn:aws:bedrock:us-east-1:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43maccount_id\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:inference-profile/us.amazon.nova-pro-v1:0\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m         },\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKNOWLEDGE_BASE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m     },\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Optional session ID can help improve results for follow-up questions:\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# sessionId='string'\u001b[39;00m\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlain text response:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'account_id' is not defined"
     ]
    }
   ],
   "source": [
    "# Use the RetrieveAndGenerate API with Nova Pro model to query the knowledge base\n",
    "rag_resp = bedrock_agent_runtime.retrieve_and_generate(\n",
    "    input={\"text\": query},\n",
    "    retrieveAndGenerateConfiguration={\n",
    "        \"knowledgeBaseConfiguration\": {\n",
    "            \"knowledgeBaseId\": knowledge_base_id,\n",
    "            \"modelArn\": f\"arn:aws:bedrock:us-east-1:{account_id}:inference-profile/us.amazon.nova-pro-v1:0\",\n",
    "        },\n",
    "        \"type\": \"KNOWLEDGE_BASE\",\n",
    "    },\n",
    "    # Optional session ID can help improve results for follow-up questions:\n",
    "    # sessionId='string'\n",
    ")\n",
    "\n",
    "print(\"Plain text response:\")\n",
    "print(\"--------------------\")\n",
    "print(rag_resp[\"output\"][\"text\"], end=\"\\n\\n\\n\")\n",
    "\n",
    "print(\"Full API output:\")\n",
    "print(\"----------------\")\n",
    "rag_resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the full API response from the above cell, the `RetrieveAndGenerate` action provides:\n",
    "\n",
    "- The final text answer\n",
    "- The `retrievedReferences` from the search engine\n",
    "- Specific `citations` localizing which references should be cited by different parts of the text answer\n",
    "\n",
    "\n",
    "It's also possible to run **only the retrieval** through the API, and skip the generative answer synthesis step - as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"content\": {\n",
      "      \"text\": \"The economy of Victoria is highly diversified: service sectors including financial and property services, health, education, wholesale, retail, hospitality and manufacturing constitute the majority of employment. Victoria's total gross state product (GSP) is ranked second in Australia, although Victoria is ranked fourth in terms of GSP per capita because of its limited mining activity. Culturally, Melbourne is home to a number of museums, art galleries and theatres and is also described as the \\\"sporting capital of Australia\\\". The Melbourne Cricket Ground is the largest stadium in Australia, and the host of the 1956 Summer Olympics and the 2006 Commonwealth Games. The ground is also considered the \\\"spiritual home\\\" of Australian cricket and Australian rules football, and hosts the grand final of the Australian Football League (AFL) each year, usually drawing crowds of over 95,000 people. Victoria includes eight public universities, with the oldest, the University of Melbourne, having been founded in 1853.  Immigrants arrived from all over the world to search for gold, especially from Ireland and China. Many Chinese miners worked in Victoria, and their legacy is particularly strong in Bendigo and its environs.\",\n",
      "      \"type\": \"TEXT\"\n",
      "    },\n",
      "    \"location\": {\n",
      "      \"s3Location\": {\n",
      "        \"uri\": \"s3://eval-369776982489-us-east-1/bedrock-rag-eval/corpus/Victoria_(Australia).txt\"\n",
      "      },\n",
      "      \"type\": \"S3\"\n",
      "    },\n",
      "    \"metadata\": {\n",
      "      \"x-amz-bedrock-kb-source-uri\": \"s3://eval-369776982489-us-east-1/bedrock-rag-eval/corpus/Victoria_(Australia).txt\",\n",
      "      \"x-amz-bedrock-kb-chunk-id\": \"1%3A0%3Ap5Dtl5gBLrQ4pVKT98r6\",\n",
      "      \"x-amz-bedrock-kb-data-source-id\": \"WHX9MOUSM3\"\n",
      "    },\n",
      "    \"score\": 0.6024491\n",
      "  },\n",
      "  {\n",
      "    \"content\": {\n",
      "      \"text\": \"Prior to the 2006 election, the Legislative Council consisted of 44 members elected to eight-year terms from 22 two-member electorates.  The centre-left Australian Labor Party (ALP), the centre-right Liberal Party of Australia, the rural-based National Party of Australia, and the environmentalist Australian Greens are Victoria's main political parties. Traditionally, Labor is strongest in Melbourne's working class western and northern suburbs, and the regional cities of Ballarat, Bendigo and Geelong. The Liberals' main support lies in Melbourne's more affluent eastern and outer suburbs, and some rural and regional centres. The Nationals are strongest in Victoria's North Western and Eastern rural regional areas. The Greens, who won their first lower house seats in 2014, are strongest in inner Melbourne.  About 61.1% of Victorians describe themselves as Christian. Roman Catholics form the single largest religious group in the state with 26.7% of the Victorian population, followed by Anglicans and members of the Uniting Church. Buddhism is the state's largest non-Christian religion, with 168,637 members as of the most recent census. Victoria is also home of 152,775 Muslims and 45,150 Jews. Hinduism is the fastest growing religion. Around 20% of Victorians claim no religion.\",\n",
      "      \"type\": \"TEXT\"\n",
      "    },\n",
      "    \"location\": {\n",
      "      \"s3Location\": {\n",
      "        \"uri\": \"s3://eval-369776982489-us-east-1/bedrock-rag-eval/corpus/Victoria_(Australia).txt\"\n",
      "      },\n",
      "      \"type\": \"S3\"\n",
      "    },\n",
      "    \"metadata\": {\n",
      "      \"x-amz-bedrock-kb-source-uri\": \"s3://eval-369776982489-us-east-1/bedrock-rag-eval/corpus/Victoria_(Australia).txt\",\n",
      "      \"x-amz-bedrock-kb-chunk-id\": \"1%3A0%3AqZDtl5gBLrQ4pVKT98r6\",\n",
      "      \"x-amz-bedrock-kb-data-source-id\": \"WHX9MOUSM3\"\n",
      "    },\n",
      "    \"score\": 0.510356\n",
      "  },\n",
      "  {\n",
      "    \"content\": {\n",
      "      \"text\": \"Many Chinese miners worked in Victoria, and their legacy is particularly strong in Bendigo and its environs. Although there was some racism directed at them, there was not the level of anti-Chinese violence that was seen at the Lambing Flat riots in New South Wales. However, there was a riot at Buckland Valley near Bright in 1857. Conditions on the gold fields were cramped and unsanitary; an outbreak of typhoid at Buckland Valley in 1854 killed over 1,000 miners.  In November 2006, the Victorian Legislative Council elections were held under a new multi-member proportional representation system. The State of Victoria was divided into eight electorates with each electorate represented by five representatives elected by Single Transferable Vote. The total number of upper house members was reduced from 44 to 40 and their term of office is now the same as the lower house members\\u2014four years. Elections for the Victorian Parliament are now fixed and occur in November every four years. Prior to the 2006 election, the Legislative Council consisted of 44 members elected to eight-year terms from 22 two-member electorates.\",\n",
      "      \"type\": \"TEXT\"\n",
      "    },\n",
      "    \"location\": {\n",
      "      \"s3Location\": {\n",
      "        \"uri\": \"s3://eval-369776982489-us-east-1/bedrock-rag-eval/corpus/Victoria_(Australia).txt\"\n",
      "      },\n",
      "      \"type\": \"S3\"\n",
      "    },\n",
      "    \"metadata\": {\n",
      "      \"x-amz-bedrock-kb-source-uri\": \"s3://eval-369776982489-us-east-1/bedrock-rag-eval/corpus/Victoria_(Australia).txt\",\n",
      "      \"x-amz-bedrock-kb-chunk-id\": \"1%3A0%3AqJDtl5gBLrQ4pVKT98r6\",\n",
      "      \"x-amz-bedrock-kb-data-source-id\": \"WHX9MOUSM3\"\n",
      "    },\n",
      "    \"score\": 0.49922895\n",
      "  },\n",
      "  {\n",
      "    \"content\": {\n",
      "      \"text\": \"Hinduism is the fastest growing religion. Around 20% of Victorians claim no religion. Amongst those who declare a religious affiliation, church attendance is low.\",\n",
      "      \"type\": \"TEXT\"\n",
      "    },\n",
      "    \"location\": {\n",
      "      \"s3Location\": {\n",
      "        \"uri\": \"s3://eval-369776982489-us-east-1/bedrock-rag-eval/corpus/Victoria_(Australia).txt\"\n",
      "      },\n",
      "      \"type\": \"S3\"\n",
      "    },\n",
      "    \"metadata\": {\n",
      "      \"x-amz-bedrock-kb-source-uri\": \"s3://eval-369776982489-us-east-1/bedrock-rag-eval/corpus/Victoria_(Australia).txt\",\n",
      "      \"x-amz-bedrock-kb-chunk-id\": \"1%3A0%3AqpDtl5gBLrQ4pVKT98r6\",\n",
      "      \"x-amz-bedrock-kb-data-source-id\": \"WHX9MOUSM3\"\n",
      "    },\n",
      "    \"score\": 0.491627\n",
      "  },\n",
      "  {\n",
      "    \"content\": {\n",
      "      \"text\": \"They include parochial schools, a term which is often used to denote Roman Catholic schools. Other religious groups represented in the K-12 private education sector include Protestants, Jews, Muslims and the Orthodox Christians.  Private schools in Australia may be favoured for many reasons: prestige and the social status of the 'old school tie'; better quality physical infrastructure and more facilities (e.g. playing fields, swimming pools, etc.), higher-paid teachers; and/or the belief that private schools offer a higher quality of education. Some schools offer the removal of the purported distractions of co-education; the presence of boarding facilities; or stricter discipline based on their power of expulsion, a tool not readily available to government schools. Student uniforms for Australian private schools are generally stricter and more formal than in government schools - for example, a compulsory blazer. Private schools in Australia are always more expensive than their public counterparts.[citation needed]\",\n",
      "      \"type\": \"TEXT\"\n",
      "    },\n",
      "    \"location\": {\n",
      "      \"s3Location\": {\n",
      "        \"uri\": \"s3://eval-369776982489-us-east-1/bedrock-rag-eval/corpus/Private_school.txt\"\n",
      "      },\n",
      "      \"type\": \"S3\"\n",
      "    },\n",
      "    \"metadata\": {\n",
      "      \"x-amz-bedrock-kb-source-uri\": \"s3://eval-369776982489-us-east-1/bedrock-rag-eval/corpus/Private_school.txt\",\n",
      "      \"x-amz-bedrock-kb-chunk-id\": \"1%3A0%3AxZDtl5gBLrQ4pVKT_soZ\",\n",
      "      \"x-amz-bedrock-kb-data-source-id\": \"WHX9MOUSM3\"\n",
      "    },\n",
      "    \"score\": 0.45050806\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "retrieve_resp = bedrock_agent_runtime.retrieve(\n",
    "    knowledgeBaseId=knowledge_base_id,\n",
    "    retrievalQuery={\"text\": query},\n",
    ")\n",
    "print(json.dumps(retrieve_resp[\"retrievalResults\"], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up dataset and metrics for evaluation\n",
    "\n",
    "### Load Dataset\n",
    "\n",
    "For this example, we are going to use a dataset with reference input/output pairs by querying a RAG system and curating the results. See below for instruction on how to fetch your production data from Langfuse.\n",
    "\n",
    "The dataset contains the following columns:\n",
    "\n",
    "- `question`: list[str] - These are the questions your RAG pipeline will be evaluated on.\n",
    "\n",
    "- `contexts`: list[list[str]] - The contexts which were passed into the LLM to answer the question.\n",
    "\n",
    "- `answer`: list[str] - The answer generated from the RAG pipeline and given to the user.\n",
    "\n",
    "- `ground_truths`: list[list[str]] - The ground truth answer to the questions. However, this can be ignored for online evaluations since we will not have access to ground-truth data in our case.\n",
    "\n",
    "For the details of this dataset, please refer to [Exploding Gradients Dataset](https://huggingface.co/datasets/explodinggradients/fiqa/viewer/ragas_eval)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by loading the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets==2.10.1\n",
      "  Downloading datasets-2.10.1-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting pyarrow==12.0.1\n",
      "  Downloading pyarrow-12.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets==2.10.1) (1.26.4)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.10.1) (0.3.5.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.10.1) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.10.1) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.10.1) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.10.1) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.10.1) (0.70.13)\n",
      "Requirement already satisfied: fsspec>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.11.1->datasets==2.10.1) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.10.1) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.10.1) (0.34.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets==2.10.1) (23.2)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets==2.10.1) (0.18.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.10.1) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.10.1) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.10.1) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.10.1) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.10.1) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.10.1) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.10.1) (4.0.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets==2.10.1) (3.13.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets==2.10.1) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets==2.10.1) (1.1.7)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.10.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.10.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.10.1) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.10.1) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.10.1) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.10.1) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.10.1) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.10.1) (1.16.0)\n",
      "Downloading datasets-2.10.1-py3-none-any.whl (469 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-12.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.9/38.9 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyarrow, datasets\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 21.0.0\n",
      "    Uninstalling pyarrow-21.0.0:\n",
      "      Successfully uninstalled pyarrow-21.0.0\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 4.0.0\n",
      "    Uninstalling datasets-4.0.0:\n",
      "      Successfully uninstalled datasets-4.0.0\n",
      "Successfully installed datasets-2.10.1 pyarrow-12.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0.0\n"
     ]
    }
   ],
   "source": [
    "# Complete setup for script-based datasets\n",
    "%pip install datasets==2.10.1 pyarrow==12.0.1import datasets, pyarrow \n",
    "print(datasets.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68aa152f8a51474fb5dc8a800be410ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset fiqa/ragas_eval to /home/sagemaker-user/.cache/huggingface/datasets/explodinggradients___fiqa/ragas_eval/1.0.0/3dc7b639f5b4b16509a3299a2ceb78bf5fe98ee6b5fee25e7d5e4d290c88efb8...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c06eee4c3617494daceb400948ebf45e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04c8b420bcff4d828949d468b57429fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2e102116a684d8eb9e4fb02537c0cce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb9679af4339499193cf0ede37383c5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating baseline split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sagemaker-user/.cache/huggingface/datasets/downloads/20b11f7bc7d6f5376bff17ddcc67dc99098950366dee70ae19916f44aa3ae33f\n",
      "Dataset fiqa downloaded and prepared to /home/sagemaker-user/.cache/huggingface/datasets/explodinggradients___fiqa/ragas_eval/1.0.0/3dc7b639f5b4b16509a3299a2ceb78bf5fe98ee6b5fee25e7d5e4d290c88efb8. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf64469291da4e55a8e4e09eaab62642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'ground_truths', 'answer', 'contexts'],\n",
       "    num_rows: 30\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "fiqa_eval = load_dataset(\"explodinggradients/fiqa\", \"ragas_eval\")[\"baseline\"]\n",
    "fiqa_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAGAS metrics\n",
    "We're going to measure the following aspects of a RAG system. These metrics are defined in [RAGAS](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/):\n",
    "\n",
    "- [Faithfulness](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/faithfulness/): This measures the factual consistency of the generated answer against the given context.\n",
    "- [Response relevancy](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/answer_relevance/): The ResponseRelevancy metric measures how relevant a response is to the user input.\n",
    "- [Context precision](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/context_precision/): Context Precision is a metric that evaluates whether all of the ground-truth relevant items present in the contexts are ranked high. Ideally all the relevant chunks must appear at the top ranks.\n",
    "\n",
    "Checkout the [RAGAS documentation](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/) to know more about these metrics and how they work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import metrics\n",
    "from ragas.metrics import (\n",
    "    Faithfulness,\n",
    "    ResponseRelevancy,\n",
    "    LLMContextPrecisionWithoutReference,\n",
    ")\n",
    "\n",
    "# metrics you chose\n",
    "metrics = [\n",
    "    Faithfulness(),\n",
    "    ResponseRelevancy(),\n",
    "    LLMContextPrecisionWithoutReference(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.run_config import RunConfig\n",
    "from ragas.metrics.base import MetricWithLLM, MetricWithEmbeddings\n",
    "\n",
    "\n",
    "# util function to init Ragas Metrics\n",
    "def init_ragas_metrics(metrics, llm, embedding):\n",
    "    for metric in metrics:\n",
    "        if isinstance(metric, MetricWithLLM):\n",
    "            print(metric.name + \" llm\")\n",
    "            metric.llm = llm\n",
    "        if isinstance(metric, MetricWithEmbeddings):\n",
    "            print(metric.name + \" embedding\")\n",
    "            metric.embeddings = embedding\n",
    "        run_config = RunConfig()\n",
    "        metric.init(run_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to initialize the metrics with LLMs and embedding models of your choice. In this example we are going to use the Bedrock Nova Pro model and Cohere embedding English model, and use the convenience wrappers from the `langchain-aws` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faithfulness llm\n",
      "answer_relevancy llm\n",
      "answer_relevancy embedding\n",
      "llm_context_precision_without_reference llm\n"
     ]
    }
   ],
   "source": [
    "from langchain_aws import BedrockEmbeddings, ChatBedrockConverse\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "\n",
    "config = {\n",
    "    \"region_name\": \"us-east-1\",  # E.g. \"us-east-1\"\n",
    "    \"llm\": \"us.amazon.nova-pro-v1:0\",  # E.g you can also use the claude models \"anthropic.claude-3-5-sonnet-20241022-v2:0\"\n",
    "    \"embeddings\": \"cohere.embed-english-v3\",  # E.g or \"amazon.titan-embed-text-v2:0\"\n",
    "    \"temperature\": 0.4,\n",
    "}\n",
    "\n",
    "evaluator_llm = LangchainLLMWrapper(\n",
    "    ChatBedrockConverse(\n",
    "        region_name=config[\"region_name\"],\n",
    "        base_url=f\"https://bedrock-runtime.{config['region_name']}.amazonaws.com\",\n",
    "        model=config[\"llm\"],\n",
    "        temperature=config[\"temperature\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "evaluator_embeddings = LangchainEmbeddingsWrapper(\n",
    "    BedrockEmbeddings(\n",
    "        region_name=config[\"region_name\"],\n",
    "        model_id=config[\"embeddings\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "init_ragas_metrics(\n",
    "    metrics,\n",
    "    llm=evaluator_llm,\n",
    "    embedding=evaluator_embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trace eval results with Langfuse\n",
    "\n",
    "You can use model-based evaluation with Ragas in 2 ways:\n",
    "1. Score every trace: This means you will run the evaluations for each trace item. This gives you much better idea of how each call made to your RAG pipelines is performing, but please be mindful of the cost.\n",
    "\n",
    "2. Score with sampling: In this method we will take random samples of traces on a periodic basis and score them. This brings down the cost and gives you a rough estimate the performance of your app but may miss out on important samples.\n",
    "\n",
    "In this example, we will demonstrate both solutions using prebuilt dataset and a live RAG pipeline with Bedrock Knowlegebase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score every trace\n",
    "\n",
    "Lets take a small example of a single trace and see how you can score that with Ragas. We first define a utility function to score your trace with the metrics you chose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.dataset_schema import SingleTurnSample\n",
    "\n",
    "\n",
    "async def score_with_ragas(query, chunks, answer, metrics):\n",
    "    scores = {}\n",
    "    for metric in metrics:\n",
    "        sample = SingleTurnSample(\n",
    "            user_input=query,\n",
    "            retrieved_contexts=chunks,\n",
    "            response=answer,\n",
    "        )\n",
    "        print(f\"calculating {metric.name}\")\n",
    "        scores[metric.name] = await metric.single_turn_ascore(sample)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring sample dataset item\n",
    "\n",
    "You compute the score with each request. Below we will go through a dummy application that does the following steps:\n",
    "\n",
    "- Gets a question from the user\n",
    "- Fetch context from the database or vector store that can be used to answer the question from the user\n",
    "- Pass the question and the contexts to the LLM to generate the answer\n",
    "\n",
    "In this case we are demonstrating the use of the Langfuse Python [low-level SDK](https://langfuse.com/docs/sdk/python/low-level-sdk) to log the traces with more granular controls. You can also see an example with the [decorator](https://langfuse.com/docs/sdk/python/decorators) in the later section or read more about them the [langfuse documentation](https://langfuse.com/docs/sdk/overview)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating faithfulness\n",
      "calculating answer_relevancy\n",
      "calculating llm_context_precision_without_reference\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'faithfulness': 1.0,\n",
       " 'answer_relevancy': 0.7153084108792154,\n",
       " 'llm_context_precision_without_reference': 0.9999999999}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start a new trace when you get a question\n",
    "row = fiqa_eval[0]\n",
    "question = row[\"question\"]\n",
    "contexts = row[\"contexts\"]\n",
    "answer = row[\"answer\"]\n",
    "\n",
    "\n",
    "# Create trace with proper input\n",
    "trace = langfuse.trace(\n",
    "    name=\"rag-fiqa\",\n",
    "    input={\"question\": question},\n",
    "    metadata={\"dataset\": \"fiqa\"}\n",
    ")\n",
    "\n",
    "# Create retrieval span and properly end it\n",
    "retrieval_span = trace.span(\n",
    "    name=\"retrieval\",\n",
    "    input={\"question\": question}\n",
    ")\n",
    "retrieval_span.end(output={\"contexts\": contexts})\n",
    "\n",
    "# Create generation and properly end it\n",
    "generation = trace.generation(\n",
    "    name=\"generation\",\n",
    "    input={\"question\": question, \"contexts\": contexts}\n",
    ")\n",
    "generation.end(output={\"answer\": answer})\n",
    "\n",
    "# End the trace with final output\n",
    "trace.update(output={\"answer\": answer})\n",
    "\n",
    "# compute scores for the question, context, answer tuple\n",
    "ragas_scores = await score_with_ragas(question, contexts, answer, metrics)\n",
    "ragas_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now you can see this is traced in langfuse but with no score attached, we can check it in the Langfuse UI at:\n",
      "http://langfu-loadb-w7udau70n77l-1274730217.ap-southeast-1.elb.amazonaws.com\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Now you can see this is traced in langfuse but with no score attached, we can check it in the Langfuse UI at:\\n{os.environ['LANGFUSE_HOST']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then attach the scores to the trace by running the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in metrics:\n",
    "      langfuse.score(\n",
    "          name=m.name,\n",
    "          value=ragas_scores[m.name],\n",
    "          trace_id=trace.id\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the score is attached\n",
    "\n",
    "![](images/bedrock-kbs/04e-langfuse-single-eval-trace-score.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring RAG\n",
    "We have already setup the Bedrock Knowledge Base in the first section, we can now **evaluate** the quality of its results against a test dataset - to help us **optimize** the configuration for high quality and low cost.\n",
    "\n",
    "First, let's load the sample dataset of questions, reference answers, and their source documents (to find more of how to prepare this dataset, please see more details in [this github](https://github.com/aws-samples/llm-evaluation-methodology/blob/main/datasets/Prepare-SQuAD.ipynb)):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>question</th>\n",
       "      <th>question_id</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
       "      <td>Normans</td>\n",
       "      <td>In what country is Normandy located?</td>\n",
       "      <td>56ddde6b9a695914005b9628</td>\n",
       "      <td>[France]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Computational complexity theory is a branch of...</td>\n",
       "      <td>Computational_complexity_theory</td>\n",
       "      <td>What branch of theoretical computer science de...</td>\n",
       "      <td>56e16182e3433e1400422e28</td>\n",
       "      <td>[Computational complexity theory]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Southern California, often abbreviated SoCal, ...</td>\n",
       "      <td>Southern_California</td>\n",
       "      <td>What is Southern California often abbreviated as?</td>\n",
       "      <td>5705e26d75f01819005e76d4</td>\n",
       "      <td>[SoCal]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Formed in November 1990 by the equal merger of...</td>\n",
       "      <td>Sky_(United_Kingdom)</td>\n",
       "      <td>What company was formed by the merger of Sky T...</td>\n",
       "      <td>57092322efce8f15003a7db0</td>\n",
       "      <td>[BSkyB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The economy of Victoria is highly diversified:...</td>\n",
       "      <td>Victoria_(Australia)</td>\n",
       "      <td>What kind of economy does Victoria have?</td>\n",
       "      <td>570d2417fed7b91900d45c3d</td>\n",
       "      <td>[diversified, highly diversified]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Huguenot numbers peaked near an estimated two ...</td>\n",
       "      <td>Huguenot</td>\n",
       "      <td>Where was France's Huguenot population largely...</td>\n",
       "      <td>57105da9a58dae1900cd699e</td>\n",
       "      <td>[the southern and central parts of France, sou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Steam engines are external combustion engines,...</td>\n",
       "      <td>Steam_engine</td>\n",
       "      <td>Along with geothermal and nuclear, what is a n...</td>\n",
       "      <td>57112686b654c5140001fbd3</td>\n",
       "      <td>[solar, solar power, solar power, nuclear powe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Oxygen is a chemical element with symbol O and...</td>\n",
       "      <td>Oxygen</td>\n",
       "      <td>The atomic number of the periodic table for ox...</td>\n",
       "      <td>571a484210f8ca1400304fbd</td>\n",
       "      <td>[8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The 1973 oil crisis began in October 1973 when...</td>\n",
       "      <td>1973_oil_crisis</td>\n",
       "      <td>When did the 1973 oil crisis begin?</td>\n",
       "      <td>5725b33f6a3fe71400b8952d</td>\n",
       "      <td>[October 1973, October, 1973]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>European Union law is a body of treaties and l...</td>\n",
       "      <td>European_Union_law</td>\n",
       "      <td>What is European Union Law?</td>\n",
       "      <td>5725b7f389a1e219009abd5d</td>\n",
       "      <td>[a body of treaties and legislation, a body of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 doc  \\\n",
       "0  The Normans (Norman: Nourmands; French: Norman...   \n",
       "1  Computational complexity theory is a branch of...   \n",
       "2  Southern California, often abbreviated SoCal, ...   \n",
       "3  Formed in November 1990 by the equal merger of...   \n",
       "4  The economy of Victoria is highly diversified:...   \n",
       "5  Huguenot numbers peaked near an estimated two ...   \n",
       "6  Steam engines are external combustion engines,...   \n",
       "7  Oxygen is a chemical element with symbol O and...   \n",
       "8  The 1973 oil crisis began in October 1973 when...   \n",
       "9  European Union law is a body of treaties and l...   \n",
       "\n",
       "                            doc_id  \\\n",
       "0                          Normans   \n",
       "1  Computational_complexity_theory   \n",
       "2              Southern_California   \n",
       "3             Sky_(United_Kingdom)   \n",
       "4             Victoria_(Australia)   \n",
       "5                         Huguenot   \n",
       "6                     Steam_engine   \n",
       "7                           Oxygen   \n",
       "8                  1973_oil_crisis   \n",
       "9               European_Union_law   \n",
       "\n",
       "                                            question  \\\n",
       "0               In what country is Normandy located?   \n",
       "1  What branch of theoretical computer science de...   \n",
       "2  What is Southern California often abbreviated as?   \n",
       "3  What company was formed by the merger of Sky T...   \n",
       "4           What kind of economy does Victoria have?   \n",
       "5  Where was France's Huguenot population largely...   \n",
       "6  Along with geothermal and nuclear, what is a n...   \n",
       "7  The atomic number of the periodic table for ox...   \n",
       "8                When did the 1973 oil crisis begin?   \n",
       "9                        What is European Union Law?   \n",
       "\n",
       "                question_id                                            answers  \n",
       "0  56ddde6b9a695914005b9628                                           [France]  \n",
       "1  56e16182e3433e1400422e28                  [Computational complexity theory]  \n",
       "2  5705e26d75f01819005e76d4                                            [SoCal]  \n",
       "3  57092322efce8f15003a7db0                                            [BSkyB]  \n",
       "4  570d2417fed7b91900d45c3d                  [diversified, highly diversified]  \n",
       "5  57105da9a58dae1900cd699e  [the southern and central parts of France, sou...  \n",
       "6  57112686b654c5140001fbd3  [solar, solar power, solar power, nuclear powe...  \n",
       "7  571a484210f8ca1400304fbd                                                [8]  \n",
       "8  5725b33f6a3fe71400b8952d                      [October 1973, October, 1973]  \n",
       "9  5725b7f389a1e219009abd5d  [a body of treaties and legislation, a body of...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df = pd.read_json(\"datasets/qa.manifest.jsonl\", lines=True)\n",
    "dataset_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Records in this dataset include:\n",
    "\n",
    "- (`doc`) The full text of the source document for this example\n",
    "- (`doc_id`) A unique identifier for the source document\n",
    "- (`question`) The user question to be asked\n",
    "- (`question_id`) A unique identifier for the question\n",
    "- (`answers`) A list of (possibly multiple) reference 'correct' answers, supported by the document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in [Ragas' API Reference](https://docs.ragas.io/en/latest/references/evaluation.html), records in Ragas evaluation datasets typically include:\n",
    "\n",
    "- The `question` that was asked\n",
    "- The `answer` the system generated\n",
    "- The actual text `contexts` the answer was based on (i.e. snippets of document text retrieved by the search engine)\n",
    "- The `ground_truth` answer(s)\n",
    "\n",
    "Here we will integrate [Langfuse Tracking](https://langfuse.com/docs/tracing) into the RAG pipeline with the Langfuse Python SDK using the `@observe()` decorator.\n",
    "\n",
    "We can run an example question through the Bedrock KB retrieve and generate pipeline as shown below, and extract the references ready to calculate metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse.decorators import observe, langfuse_context\n",
    "\n",
    "\n",
    "@observe(name=\"Knowledge Base Retrieve and Generate\")\n",
    "def retrieve_and_generate(\n",
    "    question: str,\n",
    "    kb_id: str,\n",
    "    generate_model_arn: str = f\"arn:aws:bedrock:us-east-1:{account_id}:inference-profile/us.amazon.nova-pro-v1:0\",\n",
    "    **kwargs,\n",
    "):\n",
    "    rag_resp = bedrock_agent_runtime.retrieve_and_generate(\n",
    "        input={\"text\": question},\n",
    "        retrieveAndGenerateConfiguration={\n",
    "            \"knowledgeBaseConfiguration\": {\n",
    "                \"knowledgeBaseId\": kb_id,\n",
    "                \"modelArn\": generate_model_arn,\n",
    "            },\n",
    "            \"type\": \"KNOWLEDGE_BASE\",\n",
    "        },\n",
    "    )\n",
    "    answer = rag_resp[\"output\"][\"text\"]\n",
    "\n",
    "    # Fetch flat list of references from the nested citations -> retrievedReferences:\n",
    "    all_refs = [\n",
    "        r for cite in rag_resp[\"citations\"] for r in cite[\"retrievedReferences\"]\n",
    "    ]\n",
    "    contexts = [r[\"content\"][\"text\"] for r in all_refs]\n",
    "    ref_s3uris = [r[\"location\"][\"s3Location\"][\"uri\"] for r in all_refs]\n",
    "    # Map e.g. 's3://.../doc_id.txt' to 'doc_id':\n",
    "    ref_ids = [uri.rpartition(\"/\")[2].rpartition(\".\")[0] for uri in ref_s3uris]\n",
    "\n",
    "    # Log additional data to the trace\n",
    "    langfuse_context.update_current_observation(\n",
    "        input={\"question\": question, \"contexts\": contexts},\n",
    "        output=answer,\n",
    "        model=\"us.amazon.nova-pro-v1:0\",\n",
    "        session_id=\"kb-rag-session\",\n",
    "        tags=[\"dev\"],\n",
    "        metadata=kwargs,\n",
    "    )\n",
    "\n",
    "    # Get the trace ID for independent scoring\n",
    "    trace_id = langfuse_context.get_current_trace_id()\n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"retrieved_doc_ids\": ref_ids,\n",
    "        \"retrieved_doc_texts\": contexts,\n",
    "        \"trace_id\": trace_id,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run RAG as requests come in and score the results immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse.decorators import observe, langfuse_context\n",
    "from asyncio import run\n",
    "\n",
    "\n",
    "@observe(name=\"Knowledge Base Pipeline\")\n",
    "def rag_pipeline(\n",
    "    question,\n",
    "    user_id: Optional[str] = None,\n",
    "    session_id: Optional[str] = None,\n",
    "    kb_id: Optional[str] = None,\n",
    "    metrics: Optional[Any] = None,\n",
    "):\n",
    "\n",
    "    generated_answer = retrieve_and_generate(\n",
    "        question=question,\n",
    "        kb_id=kb_id,\n",
    "        kwargs={\"database\": \"Bedrock Knowledge Base\", \"kb_id\": kb_id},\n",
    "    )\n",
    "    contexts = generated_answer[\"retrieved_doc_texts\"]\n",
    "    answer = generated_answer[\"answer\"]\n",
    "    trace_id = generated_answer[\"trace_id\"]\n",
    "\n",
    "    score = run(score_with_ragas(question, contexts, answer=answer, metrics=metrics))\n",
    "    langfuse_context.update_current_trace(\n",
    "        user_id=user_id,\n",
    "        session_id=session_id,\n",
    "        tags=[\"dev\"],\n",
    "    )\n",
    "    for s in score:\n",
    "        langfuse.score(name=s, value=score[s], trace_id=trace_id)\n",
    "    return generated_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating faithfulness\n",
      "calculating answer_relevancy\n",
      "calculating llm_context_precision_without_reference\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answer': \"Normandy is located in France.\\n\\nThe Duchy of Normandy is situated in the northern part of present-day Upper Normandy down to the river Seine. It was established by the treaty of Saint-Clair-sur-Epte between King Charles III of West Francia and the Viking ruler Rollo in 911. The Normans, who gave their name to Normandy, were descended from Norse raiders and pirates from Denmark, Iceland, and Norway who settled in the region under Rollo's leadership. Over time, the Normans assimilated with the native Frankish and Roman-Gaulish populations, and their distinct cultural and ethnic identity emerged. The Duchy of Normandy became a significant fief of medieval France.\",\n",
       " 'retrieved_doc_ids': ['Normans',\n",
       "  'Normans',\n",
       "  'Normans',\n",
       "  'Normans',\n",
       "  'Normans',\n",
       "  'Normans',\n",
       "  'Normans',\n",
       "  'Normans',\n",
       "  'Normans'],\n",
       " 'retrieved_doc_texts': ['Norman cultural and military influence spread from these new European centres to the Crusader states of the Near East, where their prince Bohemond I founded the Principality of Antioch in the Levant, to Scotland and Wales in Great Britain, to Ireland, and to the coasts of north Africa and the Canary Islands.  The English name \"Normans\" comes from the French words Normans/Normanz, plural of Normant, modern French normand, which is itself borrowed from Old Low Franconian Nortmann \"Northman\" or directly from Old Norse Norðmaðr, Latinized variously as Nortmannus, Normannus, or Nordmannus (recorded in Medieval Latin, 9th century) to mean \"Norseman, Viking\".  In the course of the 10th century, the initially destructive incursions of Norse war bands into the rivers of France evolved into more permanent encampments that included local women and personal property. The Duchy of Normandy, which began in 911 as a fiefdom, was established by the treaty of Saint-Clair-sur-Epte between King Charles III of West Francia and the famed Viking ruler Rollo, and was situated in the former Frankish kingdom of Neustria.',\n",
       "  'The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  The Norman dynasty had a major political, cultural and military impact on medieval Europe and even the Near East. The Normans were famed for their martial spirit and eventually for their Christian piety, becoming exponents of the Catholic orthodoxy into which they assimilated. They adopted the Gallo-Romance language of the Frankish land they settled, their dialect becoming known as Norman, Normaund or Norman French, an important literary language.',\n",
       "  'They adopted the Gallo-Romance language of the Frankish land they settled, their dialect becoming known as Norman, Normaund or Norman French, an important literary language. The Duchy of Normandy, which they formed by treaty with the French crown, was a great fief of medieval France, and under Richard I of Normandy was forged into a cohesive and formidable principality in feudal tenure. The Normans are noted both for their culture, such as their unique Romanesque architecture and musical traditions, and for their significant military accomplishments and innovations. Norman adventurers founded the Kingdom of Sicily under Roger II after conquering southern Italy on the Saracens and Byzantines, and an expedition on behalf of their duke, William the Conqueror, led to the Norman conquest of England at the Battle of Hastings in 1066. Norman cultural and military influence spread from these new European centres to the Crusader states of the Near East, where their prince Bohemond I founded the Principality of Antioch in the Levant, to Scotland and Wales in Great Britain, to Ireland, and to the coasts of north Africa and the Canary Islands.',\n",
       "  'The treaty offered Rollo and his men the French lands between the river Epte and the Atlantic coast in exchange for their protection against further Viking incursions. The area corresponded to the northern part of present-day Upper Normandy down to the river Seine, but the Duchy would eventually extend west beyond the Seine. The territory was roughly equivalent to the old province of Rouen, and reproduced the Roman administrative structure of Gallia Lugdunensis II (part of the former Gallia Lugdunensis).  Before Rollo\\'s arrival, its populations did not differ from Picardy or the Île-de-France, which were considered \"Frankish\". Earlier Viking settlers had begun arriving in the 880s, but were divided between colonies in the east (Roumois and Pays de Caux) around the low Seine valley and in the west in the Cotentin Peninsula, and were separated by traditional pagii, where the population remained about the same with almost no foreign settlers. Rollo\\'s contingents who raided and ultimately settled Normandy and parts of the Atlantic coast included Danes, Norwegians, Norse–Gaels, Orkney Vikings, possibly Swedes, and Anglo-Danes from the English Danelaw under Norse control.',\n",
       "  'The treaty offered Rollo and his men the French lands between the river Epte and the Atlantic coast in exchange for their protection against further Viking incursions. The area corresponded to the northern part of present-day Upper Normandy down to the river Seine, but the Duchy would eventually extend west beyond the Seine. The territory was roughly equivalent to the old province of Rouen, and reproduced the Roman administrative structure of Gallia Lugdunensis II (part of the former Gallia Lugdunensis).  Before Rollo\\'s arrival, its populations did not differ from Picardy or the Île-de-France, which were considered \"Frankish\". Earlier Viking settlers had begun arriving in the 880s, but were divided between colonies in the east (Roumois and Pays de Caux) around the low Seine valley and in the west in the Cotentin Peninsula, and were separated by traditional pagii, where the population remained about the same with almost no foreign settlers. Rollo\\'s contingents who raided and ultimately settled Normandy and parts of the Atlantic coast included Danes, Norwegians, Norse–Gaels, Orkney Vikings, possibly Swedes, and Anglo-Danes from the English Danelaw under Norse control.',\n",
       "  'Norman cultural and military influence spread from these new European centres to the Crusader states of the Near East, where their prince Bohemond I founded the Principality of Antioch in the Levant, to Scotland and Wales in Great Britain, to Ireland, and to the coasts of north Africa and the Canary Islands.  The English name \"Normans\" comes from the French words Normans/Normanz, plural of Normant, modern French normand, which is itself borrowed from Old Low Franconian Nortmann \"Northman\" or directly from Old Norse Norðmaðr, Latinized variously as Nortmannus, Normannus, or Nordmannus (recorded in Medieval Latin, 9th century) to mean \"Norseman, Viking\".  In the course of the 10th century, the initially destructive incursions of Norse war bands into the rivers of France evolved into more permanent encampments that included local women and personal property. The Duchy of Normandy, which began in 911 as a fiefdom, was established by the treaty of Saint-Clair-sur-Epte between King Charles III of West Francia and the famed Viking ruler Rollo, and was situated in the former Frankish kingdom of Neustria.',\n",
       "  'The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  The Norman dynasty had a major political, cultural and military impact on medieval Europe and even the Near East. The Normans were famed for their martial spirit and eventually for their Christian piety, becoming exponents of the Catholic orthodoxy into which they assimilated. They adopted the Gallo-Romance language of the Frankish land they settled, their dialect becoming known as Norman, Normaund or Norman French, an important literary language.',\n",
       "  'The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  The Norman dynasty had a major political, cultural and military impact on medieval Europe and even the Near East. The Normans were famed for their martial spirit and eventually for their Christian piety, becoming exponents of the Catholic orthodoxy into which they assimilated. They adopted the Gallo-Romance language of the Frankish land they settled, their dialect becoming known as Norman, Normaund or Norman French, an important literary language.',\n",
       "  'They adopted the Gallo-Romance language of the Frankish land they settled, their dialect becoming known as Norman, Normaund or Norman French, an important literary language. The Duchy of Normandy, which they formed by treaty with the French crown, was a great fief of medieval France, and under Richard I of Normandy was forged into a cohesive and formidable principality in feudal tenure. The Normans are noted both for their culture, such as their unique Romanesque architecture and musical traditions, and for their significant military accomplishments and innovations. Norman adventurers founded the Kingdom of Sicily under Roger II after conquering southern Italy on the Saracens and Byzantines, and an expedition on behalf of their duke, William the Conqueror, led to the Norman conquest of England at the Battle of Hastings in 1066. Norman cultural and military influence spread from these new European centres to the Crusader states of the Near East, where their prince Bohemond I founded the Principality of Antioch in the Levant, to Scotland and Wales in Great Britain, to Ireland, and to the coasts of north Africa and the Canary Islands.'],\n",
       " 'trace_id': '91b29477-a5f2-4cc7-9f59-f805a9cc8897'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = rag_pipeline(dataset_df.iloc[0][\"question\"], kb_id=knowledge_base_id, metrics=metrics)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring with sampling\n",
    "\n",
    "Scoring every production trace can be time-consuming and costly depending on your application architecture and traffic. In that case, it's better to start off with a sampling method. Decide a timespan you want to run the batch process and the number of traces you want to sample from that time slice. Create a dataset and call ragas.evaluate to analyze the result.\n",
    "\n",
    "You can run this periodically to keep track of how the scores are changing across timeslices and figure out if there are any discrepancies.\n",
    "\n",
    "We will evaluate the existing results generated previously by the `retrieve_and_generate()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate 10 production traces by running RAG on the first 10 questions in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_generated_outputs = [\n",
    "    retrieve_and_generate(\n",
    "        question=rec.question,\n",
    "        kb_id=knowledge_base_id,\n",
    "        kwargs={\"database\": \"Bedrock Knowledge Base\", \"kb_id\": knowledge_base_id},\n",
    "    )\n",
    "    for _, rec in dataset_df.head(10).iterrows()\n",
    "]\n",
    "rag_generated_outputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the results are uploaded to langfuse you can retrieve it as needed with this handy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse.api.resources.commons.types.trace_with_details import TraceWithDetails\n",
    "\n",
    "def get_traces(\n",
    "    limit: int = 5,\n",
    "    name: Optional[str] = None,\n",
    "    user_id: Optional[str] = None,\n",
    "    session_id: Optional[str] = None,\n",
    "    from_timestamp: Optional[str] = None,\n",
    "    to_timestamp: Optional[str] = None,\n",
    ") -> List[TraceWithDetails]:\n",
    "    \"\"\"Query Langfuse for traces matching the given filters.\n",
    "    See https://langfuse.com/docs/query-traces for more details.\"\"\"\n",
    "\n",
    "    all_data = []\n",
    "    page = 1\n",
    "\n",
    "    while True:\n",
    "        response = langfuse.fetch_traces(\n",
    "            page=page,\n",
    "            name=name,\n",
    "            user_id=user_id,\n",
    "            session_id=session_id,\n",
    "            from_timestamp=from_timestamp,\n",
    "            to_timestamp=to_timestamp,\n",
    "        )\n",
    "        if not response.data:\n",
    "            break\n",
    "        page += 1\n",
    "        all_data.extend(response.data)\n",
    "        if len(all_data) > limit:\n",
    "            break\n",
    "\n",
    "    return all_data[:limit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 0 traces from 0 filtered traces\n"
     ]
    }
   ],
   "source": [
    "from random import sample\n",
    "\n",
    "NUM_TRACES_TO_SAMPLE = 3\n",
    "traces = get_traces(name=\"Knowledge Base Retrieve and Generate\", limit=10)\n",
    "if len(traces) > NUM_TRACES_TO_SAMPLE:\n",
    "    traces_sample = sample(traces, NUM_TRACES_TO_SAMPLE)\n",
    "else:\n",
    "    traces_sample = traces\n",
    "\n",
    "print(f\"Sampled {len(traces_sample)} traces from {len(traces)} filtered traces\")\n",
    "for trace in traces_sample:\n",
    "    print(f\"Trace ID: {trace.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets make a batch and score it. Ragas uses huggingface dataset object to build the dataset and run the evaluation. If you run this on your own production data, use the right keys to extract the question, contexts and answer from the trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score on a sample\n",
    "evaluation_batch = {\n",
    "    \"question\": [],\n",
    "    \"contexts\": [],\n",
    "    \"answer\": [],\n",
    "    \"trace_id\": [],\n",
    "}\n",
    "\n",
    "for sample in traces_sample:\n",
    "    evaluation_batch[\"question\"].append(sample.input[\"question\"])\n",
    "    evaluation_batch[\"contexts\"].append(sample.input[\"contexts\"])\n",
    "    evaluation_batch[\"answer\"].append(sample.output)\n",
    "    evaluation_batch[\"trace_id\"].append(sample.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the ragas evaluate function to score an entire dataset instead of single turn. See [Ragas evaluate](https://docs.ragas.io/en/latest/references/evaluate/) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_batch.dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Dataset' object has no attribute 'to_list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Faithfulness, ResponseRelevancy\n\u001b[1;32m      6\u001b[0m ds \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_dict(evaluation_batch)\n\u001b[0;32m----> 7\u001b[0m evals_results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluator_llm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluator_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mFaithfulness\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mResponseRelevancy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m evals_results\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ragas/_analytics.py:227\u001b[0m, in \u001b[0;36mtrack_was_completed.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[1;32m    226\u001b[0m     track(IsCompleteEvent(event_type\u001b[38;5;241m=\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, is_completed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[0;32m--> 227\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     track(IsCompleteEvent(event_type\u001b[38;5;241m=\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, is_completed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ragas/evaluation.py:173\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(dataset, metrics, llm, embeddings, experiment_name, callbacks, run_config, token_usage_parser, raise_exceptions, column_map, show_progress, batch_size, _run_id, _pbar)\u001b[0m\n\u001b[1;32m    171\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m convert_v1_to_v2_dataset(dataset)\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;66;03m# validation\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m EvaluationDataset\u001b[38;5;241m.\u001b[39mfrom_list(\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_list\u001b[49m())\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, EvaluationDataset):\n\u001b[1;32m    176\u001b[0m     validate_required_columns(dataset, metrics)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Dataset' object has no attribute 'to_list'"
     ]
    }
   ],
   "source": [
    "# run ragas evaluate\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import Faithfulness, ResponseRelevancy\n",
    "\n",
    "ds = Dataset.from_dict(evaluation_batch)\n",
    "evals_results = evaluate(\n",
    "    ds,\n",
    "    llm=evaluator_llm,\n",
    "    embeddings=evaluator_embeddings,\n",
    "    metrics=[Faithfulness(), ResponseRelevancy()],\n",
    ")\n",
    "evals_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that is it! You can see the scores over a time period. Let's render the results in a dataframe to see the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evals_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mevals_results\u001b[49m\u001b[38;5;241m.\u001b[39mto_pandas()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# add the langfuse trace_id to the result dataframe\u001b[39;00m\n\u001b[1;32m      4\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrace_id\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m ds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrace_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'evals_results' is not defined"
     ]
    }
   ],
   "source": [
    "df = evals_results.to_pandas()\n",
    "\n",
    "# add the langfuse trace_id to the result dataframe\n",
    "df[\"trace_id\"] = ds[\"trace_id\"]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also push the scores back into Langfuse and attach them to the traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in df.iterrows():\n",
    "    for metric_name in [\"faithfulness\", \"answer_relevancy\"]:\n",
    "        langfuse.score(\n",
    "            name=metric_name, value=row[metric_name], trace_id=row[\"trace_id\"]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now go back to the Langfuse console and check the updated scores in the traces.\n",
    "\n",
    "![](images/bedrock-kbs/score-with-sampling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratuations!\n",
    "You have successfully finished Lab 2.\n",
    "\n",
    "If you are at an AWS event, you can return to the workshop studio for additional instructions before moving into the next lab, where we will explore model-based evaluation and guardrails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Retrieval-Augmented Generation (RAG) pipelines with Ragas and Langfuse\n",
    "\n",
    "In this notebook we'll explore ways to evaluate the quality of Retrieval-Augmented Generation (RAG) pipelines with the opensource tools like [RAGAS](https://docs.ragas.io/en/v0.1.21/index.html) and leverage the features in [Langfuse](https://langfuse.com/) to manage and trace the RAG pipelines with traces and spans. We will create a Bedrock knowledge base and the RAG batch generation results to show offline evaluation and scoring.\n",
    "\n",
    "> ℹ️ Note: This notebook requires user configurations for some steps. \n",
    ">\n",
    "> When a cell requires user configurations, you will see a message like this callout with the 👉 emoji.\n",
    ">\n",
    "> Pay attention to the instructions with the 👉 emoji and perform the configurations in the AWS Console or in the corresponding cell before running the code cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "\n",
    "> If you haven't selected the kernel, please click on the \"Select Kernel\" button at the upper right corner, select Python Environments and choose \".venv (Python 3.9.20) .venv/bin/python Recommended\".\n",
    "\n",
    "> To execute each notebook cell, press Shift + Enter.\n",
    "\n",
    "> ℹ️ You can **skip these prerequisite steps** if you're in an instructor-led workshop using temporary accounts provided by AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional permissions for Amazon OpenSearch\n",
    "\n",
    "To complete the manual Bedrock Knowledge setup steps in this notebook, your **AWS Console user/role** will need:\n",
    "\n",
    "- [Permissions to work with Amazon OpenSearch vector collections](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless-vector-search.html)\n",
    "- Permission to **create IAM roles** and attach policies to them, including: `iam:AttachRolePolicy`, `iam:CreateRole`, `iam:DetachRolePolicy`, `iam:GetRole`, `iam:PassRole`, `iam:CreatePolicy`, `iam:CreatePolicyVersion`, and `iam:DeletePolicyVersion`.\n",
    "\n",
    "> ℹ️ **Note:** In testing, we saw `NetworkError` issues when attempting to create Bedrock KBs using only the above-linked `aoss` policy statements. This was resolved by granting `aoss:*` on `*` instead, but you should consider reducing these permissions before using in production environments.\n",
    "\n",
    "Refer to the [AWS Console for Identity and Access Management (IAM)](https://console.aws.amazon.com/iam/home?#/home) to grant permissions to your user or role."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies and Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.18.0)\n",
      "Collecting datasets\n",
      "  Downloading datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting ragas\n",
      "  Downloading ragas-0.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: python-dotenv in /opt/conda/lib/python3.10/site-packages (1.1.1)\n",
      "Collecting langchain-aws\n",
      "  Downloading langchain_aws-0.2.30-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.10/site-packages (1.40.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Using cached pyarrow-21.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.4)\n",
      "Collecting tqdm>=4.66.3 (from datasets)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2023.6.0)\n",
      "Collecting huggingface-hub>=0.24.0 (from datasets)\n",
      "  Using cached huggingface_hub-0.34.3-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: tiktoken in /opt/conda/lib/python3.10/site-packages (from ragas) (0.5.2)\n",
      "Requirement already satisfied: langchain in /opt/conda/lib/python3.10/site-packages (from ragas) (0.1.9)\n",
      "Requirement already satisfied: langchain-core in /opt/conda/lib/python3.10/site-packages (from ragas) (0.1.42)\n",
      "Requirement already satisfied: langchain-community in /opt/conda/lib/python3.10/site-packages (from ragas) (0.0.32)\n",
      "Collecting langchain_openai (from ragas)\n",
      "  Using cached langchain_openai-0.3.28-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.10/site-packages (from ragas) (1.6.0)\n",
      "Collecting appdirs (from ragas)\n",
      "  Using cached appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: pydantic>=2 in /opt/conda/lib/python3.10/site-packages (from ragas) (2.11.7)\n",
      "Requirement already satisfied: openai>1 in /opt/conda/lib/python3.10/site-packages (from ragas) (1.99.1)\n",
      "Requirement already satisfied: diskcache>=5.6.3 in /opt/conda/lib/python3.10/site-packages (from ragas) (5.6.3)\n",
      "Collecting langchain-core (from ragas)\n",
      "  Downloading langchain_core-0.3.72-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: botocore<1.41.0,>=1.40.4 in /opt/conda/lib/python3.10/site-packages (from boto3) (1.40.4)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /opt/conda/lib/python3.10/site-packages (from boto3) (0.13.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore<1.41.0,>=1.40.4->boto3) (2.9.0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.10/site-packages (from botocore<1.41.0,>=1.40.4->boto3) (1.26.18)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.9.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (4.14.1)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub>=0.24.0->datasets)\n",
      "  Downloading hf_xet-1.1.7-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (703 bytes)\n",
      "Collecting langsmith>=0.3.45 (from langchain-core->ragas)\n",
      "  Downloading langsmith-0.4.13-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain-core->ragas) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core->ragas) (1.33)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from openai>1->ragas) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from openai>1->ragas) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from openai>1->ragas) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from openai>1->ragas) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from openai>1->ragas) (1.3.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2->ragas) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2->ragas) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2->ragas) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.2.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain->ragas) (1.4.49)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain->ragas) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain->ragas) (0.6.4)\n",
      "INFO: pip is looking at multiple versions of langchain to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain (from ragas)\n",
      "  Downloading langchain-0.3.27-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.9 (from langchain->ragas)\n",
      "  Downloading langchain_text_splitters-0.3.9-py3-none-any.whl.metadata (1.9 kB)\n",
      "INFO: pip is looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain-community (from ragas)\n",
      "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from langchain-community->ragas) (2.10.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from langchain-community->ragas) (0.4.1)\n",
      "Collecting tiktoken (from ragas)\n",
      "  Downloading tiktoken-0.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken->ragas) (2023.12.25)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.9.4)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai>1->ragas) (1.2.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain->ragas) (3.21.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain->ragas) (0.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai>1->ragas) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>1->ragas) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core->ragas) (2.4)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith>=0.3.45->langchain-core->ragas) (3.9.15)\n",
      "Collecting requests-toolbelt>=1.0.0 (from langsmith>=0.3.45->langchain-core->ragas)\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard>=0.23.0 (from langsmith>=0.3.45->langchain-core->ragas)\n",
      "  Downloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.41.0,>=1.40.4->boto3) (1.16.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain->ragas) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain->ragas) (1.0.0)\n",
      "Downloading datasets-4.0.0-py3-none-any.whl (494 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.8/494.8 kB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ragas-0.3.0-py3-none-any.whl (190 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_aws-0.2.30-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.9/133.9 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached huggingface_hub-0.34.3-py3-none-any.whl (558 kB)\n",
      "Downloading langchain_core-0.3.72-py3-none-any.whl (442 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.8/442.8 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached pyarrow-21.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (42.7 MB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading langchain-0.3.27-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m107.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_openai-0.3.28-py3-none-any.whl (70 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.1.7-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m112.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_text_splitters-0.3.9-py3-none-any.whl (33 kB)\n",
      "Downloading langsmith-0.4.13-py3-none-any.whl (372 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m372.7/372.7 kB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m130.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: appdirs, zstandard, tqdm, pyarrow, hf-xet, dill, tiktoken, requests-toolbelt, huggingface-hub, langsmith, langchain-core, datasets, langchain-text-splitters, langchain_openai, langchain-aws, langchain, langchain-community, ragas\n",
      "  Attempting uninstall: zstandard\n",
      "    Found existing installation: zstandard 0.22.0\n",
      "    Uninstalling zstandard-0.22.0:\n",
      "      Successfully uninstalled zstandard-0.22.0\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.2\n",
      "    Uninstalling tqdm-4.66.2:\n",
      "      Successfully uninstalled tqdm-4.66.2\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 12.0.1\n",
      "    Uninstalling pyarrow-12.0.1:\n",
      "      Successfully uninstalled pyarrow-12.0.1\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.4.0\n",
      "    Uninstalling dill-0.4.0:\n",
      "      Successfully uninstalled dill-0.4.0\n",
      "  Attempting uninstall: tiktoken\n",
      "    Found existing installation: tiktoken 0.5.2\n",
      "    Uninstalling tiktoken-0.5.2:\n",
      "      Successfully uninstalled tiktoken-0.5.2\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface_hub 0.22.2\n",
      "    Uninstalling huggingface_hub-0.22.2:\n",
      "      Successfully uninstalled huggingface_hub-0.22.2\n",
      "  Attempting uninstall: langsmith\n",
      "    Found existing installation: langsmith 0.1.47\n",
      "    Uninstalling langsmith-0.1.47:\n",
      "      Successfully uninstalled langsmith-0.1.47\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.1.42\n",
      "    Uninstalling langchain-core-0.1.42:\n",
      "      Successfully uninstalled langchain-core-0.1.42\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.18.0\n",
      "    Uninstalling datasets-2.18.0:\n",
      "      Successfully uninstalled datasets-2.18.0\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.1.9\n",
      "    Uninstalling langchain-0.1.9:\n",
      "      Successfully uninstalled langchain-0.1.9\n",
      "  Attempting uninstall: langchain-community\n",
      "    Found existing installation: langchain-community 0.0.32\n",
      "    Uninstalling langchain-community-0.0.32:\n",
      "      Successfully uninstalled langchain-community-0.0.32\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "jupyter-ai 2.12.0 requires faiss-cpu, which is not installed.\n",
      "autogluon-multimodal 0.8.2 requires jsonschema<4.18,>=4.14, but you have jsonschema 4.25.0 which is incompatible.\n",
      "autogluon-multimodal 0.8.2 requires Pillow<9.6,>=9.3, but you have pillow 11.3.0 which is incompatible.\n",
      "gluonts 0.13.7 requires pydantic~=1.7, but you have pydantic 2.11.7 which is incompatible.\n",
      "jupyter-ai-magics 2.12.0 requires langchain<0.2.0,>=0.1.0, but you have langchain 0.3.27 which is incompatible.\n",
      "sagemaker 2.214.3 requires protobuf<5.0,>=3.12, but you have protobuf 6.31.1 which is incompatible.\n",
      "spacy 3.7.3 requires typer<0.10.0,>=0.3.0, but you have typer 0.16.0 which is incompatible.\n",
      "strands-agents-tools 0.2.3 requires dill<0.5.0,>=0.4.0, but you have dill 0.3.8 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed appdirs-1.4.4 datasets-4.0.0 dill-0.3.8 hf-xet-1.1.7 huggingface-hub-0.34.3 langchain-0.3.27 langchain-aws-0.2.30 langchain-community-0.3.27 langchain-core-0.3.72 langchain-text-splitters-0.3.9 langchain_openai-0.3.28 langsmith-0.4.13 pyarrow-21.0.0 ragas-0.3.0 requests-toolbelt-1.0.0 tiktoken-0.10.0 tqdm-4.67.1 zstandard-0.23.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Uncomment the following line to install dependencies if you are not using AWS workshop environment\n",
    "%pip install  datasets ragas python-dotenv langchain-aws boto3 --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please make sure you have completed the prerequisites to setup the Langfuse project and API keys in the .env file to connect to self-hosted or cloud Langfuse environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [Langfuse documentation](https://langfuse.com/docs/get-started) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization and Authentication Check\n",
    "Run the following cells to initialize common libraries and clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Any, List, Optional\n",
    "\n",
    "# External Dependencies:\n",
    "import pandas as pd  # For working with tabular data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize AWS Bedrock clients and check models available in your account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US Anthropic Claude 3 Sonnet - us.anthropic.claude-3-sonnet-20240229-v1:0\n",
      "US Anthropic Claude 3 Opus - us.anthropic.claude-3-opus-20240229-v1:0\n",
      "US Anthropic Claude 3 Haiku - us.anthropic.claude-3-haiku-20240307-v1:0\n",
      "US Meta Llama 3.2 11B Instruct - us.meta.llama3-2-11b-instruct-v1:0\n",
      "US Meta Llama 3.2 3B Instruct - us.meta.llama3-2-3b-instruct-v1:0\n",
      "US Meta Llama 3.2 90B Instruct - us.meta.llama3-2-90b-instruct-v1:0\n",
      "US Meta Llama 3.2 1B Instruct - us.meta.llama3-2-1b-instruct-v1:0\n",
      "US Anthropic Claude 3.5 Sonnet - us.anthropic.claude-3-5-sonnet-20240620-v1:0\n",
      "US Anthropic Claude 3.5 Haiku - us.anthropic.claude-3-5-haiku-20241022-v1:0\n",
      "US Meta Llama 3.1 8B Instruct - us.meta.llama3-1-8b-instruct-v1:0\n",
      "US Meta Llama 3.1 70B Instruct - us.meta.llama3-1-70b-instruct-v1:0\n",
      "US Meta Llama 3.3 70B Instruct - us.meta.llama3-3-70b-instruct-v1:0\n",
      "US Anthropic Claude 3.5 Sonnet v2 - us.anthropic.claude-3-5-sonnet-20241022-v2:0\n",
      "US DeepSeek-R1 - us.deepseek.r1-v1:0\n",
      "US Mistral Pixtral Large 25.02 - us.mistral.pixtral-large-2502-v1:0\n",
      "US Llama 4 Scout 17B Instruct - us.meta.llama4-scout-17b-instruct-v1:0\n",
      "US Llama 4 Maverick 17B Instruct - us.meta.llama4-maverick-17b-instruct-v1:0\n",
      "US Nova Premier - us.amazon.nova-premier-v1:0\n",
      "US Claude Opus 4 - us.anthropic.claude-opus-4-20250514-v1:0\n",
      "US Anthropic Claude 3.7 Sonnet - us.anthropic.claude-3-7-sonnet-20250219-v1:0\n",
      "US Nova Pro - us.amazon.nova-pro-v1:0\n",
      "US Nova Micro - us.amazon.nova-micro-v1:0\n",
      "US Claude Sonnet 4 - us.anthropic.claude-sonnet-4-20250514-v1:0\n",
      "US Claude Opus 4.1 - us.anthropic.claude-opus-4-1-20250805-v1:0\n"
     ]
    }
   ],
   "source": [
    "import boto3  # General Python SDK for AWS (including Bedrock)\n",
    "\n",
    "# used to access Bedrock configuration\n",
    "bedrock = boto3.client(service_name=\"bedrock\", region_name=\"us-east-1\")\n",
    "\n",
    "bedrock_agent_runtime = boto3.client(\n",
    "    service_name=\"bedrock-agent-runtime\", region_name=\"us-east-1\"\n",
    ")\n",
    "\n",
    "# Check which models are available in your account\n",
    "models = bedrock.list_inference_profiles()\n",
    "for model in models[\"inferenceProfileSummaries\"]:\n",
    "    print(model[\"inferenceProfileName\"] + \" - \" + model[\"inferenceProfileId\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the Langfuse client and check credentials are valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langfuse has been set up correctly\n",
      "You can access your Langfuse instance at: http://langfu-loadb-w7udau70n77l-1274730217.ap-southeast-1.elb.amazonaws.com\n"
     ]
    }
   ],
   "source": [
    "from langfuse import Langfuse\n",
    "\n",
    "# langfuse client\n",
    "langfuse = Langfuse()\n",
    "if langfuse.auth_check():\n",
    "    print(\"Langfuse has been set up correctly\")\n",
    "    print(f\"You can access your Langfuse instance at: {os.environ['LANGFUSE_HOST']}\")\n",
    "else:\n",
    "    print(\n",
    "        \"Credentials not found or invalid. Check your Langfuse API key and host in the .env file.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the Knowledge Base\n",
    "Next, let's upload the documents to Amazon S3 and create a vector store (knowledge base) so we can perform retrieval-augmented generation (RAG) given a user query. In the following steps, we'll configure:\n",
    "\n",
    "- An Amazon S3 bucket_name to store our document corpus. \n",
    "- A folder prefix under the bucket where artifacts will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '4FDG1Y6NXN7AADB2',\n",
       "  'HostId': '0oXbw/szREtoRavSx38pXsKX7hrDM8F5fgit6PTpC0BEAoweafiP9Pqf+6sN0NvKQQ3VbJY5tr0=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': '0oXbw/szREtoRavSx38pXsKX7hrDM8F5fgit6PTpC0BEAoweafiP9Pqf+6sN0NvKQQ3VbJY5tr0=',\n",
       "   'x-amz-request-id': '4FDG1Y6NXN7AADB2',\n",
       "   'date': 'Thu, 07 Aug 2025 10:27:46 GMT',\n",
       "   'location': '/eval-369776982489-us-east-1',\n",
       "   'content-length': '0',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'Location': '/eval-369776982489-us-east-1'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3.create_bucket(Bucket=bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating bucket eval-369776982489-us-east-1\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "An error occurred (InvalidLocationConstraint) when calling the CreateBucket operation: The specified location-constraint is not valid",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 12\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 12\u001b[0m     \u001b[43ms3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead_bucket\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBucket\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbucket_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBucket \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbucket_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m exists\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/botocore/client.py:602\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/botocore/context.py:123\u001b[0m, in \u001b[0;36mwith_current_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m     hook()\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/botocore/client.py:1078\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1077\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1078\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (404) when calling the HeadBucket operation: Not Found",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClientError:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating bucket \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbucket_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m     \u001b[43ms3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_bucket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mBucket\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbucket_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCreateBucketConfiguration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLocationConstraint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mregion\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/botocore/client.py:602\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    599\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    600\u001b[0m     )\n\u001b[1;32m    601\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/botocore/context.py:123\u001b[0m, in \u001b[0;36mwith_current_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook:\n\u001b[1;32m    122\u001b[0m     hook()\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/botocore/client.py:1078\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1074\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m request_context\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1075\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror_code_override\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1076\u001b[0m     ) \u001b[38;5;129;01mor\u001b[39;00m error_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1077\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1078\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1080\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (InvalidLocationConstraint) when calling the CreateBucket operation: The specified location-constraint is not valid"
     ]
    }
   ],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "\n",
    "botosess = boto3.Session(region_name=\"us-east-1\")\n",
    "region = botosess.region_name\n",
    "account_id = boto3.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "bucket_name = f\"eval-{account_id}-{region}\"\n",
    "s3_prefix = \"bedrock-rag-eval\"\n",
    "\n",
    "# check if s3 bucket exists or not, if not, create bucket\n",
    "s3 = boto3.client(\"s3\")\n",
    "try:\n",
    "    s3.head_bucket(Bucket=bucket_name)\n",
    "    print(f\"Bucket {bucket_name} exists\")\n",
    "except ClientError:\n",
    "    print(f\"Creating bucket {bucket_name}\")\n",
    "    s3.create_bucket(\n",
    "        Bucket=bucket_name, CreateBucketConfiguration={\"LocationConstraint\": region}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the documents to Amazon S3\n",
    "\n",
    "First, we'll need to upload the sample documents to Amazon S3 - for which you can just run the code cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syncing corpus to:\n",
      "s3://eval-369776982489-us-east-1/bedrock-rag-eval/corpus/\n"
     ]
    }
   ],
   "source": [
    "corpus_s3uri = f\"s3://{bucket_name}/{s3_prefix}/corpus\"\n",
    "print(f\"Syncing corpus to:\\n{corpus_s3uri}/\")\n",
    "\n",
    "# We will use the AWS CLI to recursively sync the folder to the S3 bucket.\n",
    "!aws s3 sync --quiet ./datasets/corpus {corpus_s3uri}/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the knowledge base in AWS Console\n",
    "> 👉 This section includes steps you'll need to take manually, not just running the code cells!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to set up the actual Bedrock Knowledge Base for testing is **manually through the AWS Console**:\n",
    "\n",
    "1. First, **open** the [AWS Console for Amazon Bedrock](https://console.aws.amazon.com/bedrock/home?#/knowledge-bases) and select *Orchestration > Knowledge bases* from the left sidebar menu, as shown in the screenshot below:\n",
    "\n",
    "    > ℹ️ **Check** you're working in the correct *AWS Region* in the top right corner of the UI\n",
    "\n",
    "![KB Console](images/bedrock-kbs/01-bedrock-kb-console.png \"Screenshot of AWS Console for Amazon Bedrock Knowledge Bases, showing 'Create knowledge base' action button\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Click the **Create knowledge base** button and select **Knowledge Base with vector store**. In the screen that opens:\n",
    "\n",
    "- For **knowledge base name**, enter `example-squad-kb`\n",
    "- For **knowledge base description**, you can provide (something like) `Demo knowledge base for question answering evaluation`\n",
    "- Leave the other settings as default (allow creating a new execution role, and no tags)\n",
    "- Please chose Amazon S3 as the data source (default)\n",
    "\n",
    "Your configuration should look like the screenshot below:\n",
    "\n",
    "![KB Basics](images/bedrock-kbs/02a-create-kb-basics.png \"Screenshot of step 1 in Bedrock Knowledge Base creation workflow: with KB name, description, (create new) execution role, and (empty) tags configured. At the end of the form, a 'Next' button is visible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. In the **Next** screen, you'll configure the S3 data source.\n",
    "\n",
    "    Leave the data source as S3 and then select the bucket and prefix per you created in the previous step and use Amazon Bedrock default parser.\n",
    "\n",
    "![](images/bedrock-kbs/02b-create-kb-data-source.png \"Screenshot of Knowledge Base vector index settings including Cohere Embed Multilingual embedding model, and quick-create vector store. 'Next' button is visible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. In the **Next** screen, you'll configure the vector index:\n",
    "\n",
    "    For *embeddings model*, select `Cohere Embed Multilingual`\n",
    "\n",
    "    > ℹ️ **Check** in the [Amazon Bedrock Model Access console](https://console.aws.amazon.com/bedrock/home?#/modelaccess) that you've enabled access to this model in the current region.\n",
    "    >\n",
    "    > If needed, you should be able to select an alternative embedding model instead.\n",
    "\n",
    "    For *Vector database*, select `Quick create a new vector store`\n",
    "\n",
    "    You can find more information from this screen or the [Amazon Bedrock Developer Guide](https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base-setup.html) about the different vector stores Bedrock Knowledge Bases support. This default option will create a new [Amazon OpenSearch Serverless](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless-overview.html) cluster\n",
    "\n",
    "    Leave other settings at their defaults as shown below, and you should be ready to proceed:\n",
    "\n",
    "![](images/bedrock-kbs/02c-create-kb-index.png \"Screenshot of Knowledge Base vector index settings including Cohere Embed Multilingual embedding model, and quick-create vector store. 'Next' button is visible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Click **Next** to review your configuration, and then **Create knowledge base** to complete the process.\n",
    "\n",
    "    > ⏰ It might take **a few minutes** for the creation to complete. A progress indicator banner should be visible if you scroll up. Alternatively in a separate tab, you could check the [Amazon OpenSearch Serverless Collections console](https://console.aws.amazon.com/aos/home?#opensearch/collections) - where you should see the underlying vector collection being created.\n",
    "\n",
    "    Once your Knowledge Base is completed successfully, you'll be directed to the its detail screen as shown below:\n",
    "\n",
    "![](images/bedrock-kbs/03-kb-detail-page.png \"Detail screen for the created Amazon Bedrock Knowledge Base, showing creation success banner. Includes sections 'Knowledge base overview' (containing the KB ID, name, and other details); 'Tags' (empty); 'Data source' (one Amazon S3 data source listed); 'Embeddings model' (Cohere Embed); and an interactive 'Test knowledge base' chat sidebar on the right with a warning that some data sources have not been synced.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. As mentioned in the alert box shown ahead, your new knowledge base will not yet contain your documents until we **sync** the data source:\n",
    "\n",
    "    **Select** your S3 data source by selecting the checkbox to the left of it's name in the data sources list, and click the **Sync** button above to start the sync.\n",
    "\n",
    "    The *Status* will change to `Syncing` for a few seconds, after which it will return to `Available`\n",
    "\n",
    "![](images/bedrock-kbs/04a-kb-data-source-after-sync.png \"Screenshot of KB 'data source' section after running sync, with the data source selected and status showing as 'available'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the sync completed, your Knowledge Base should be ready to use.\n",
    "\n",
    "Optionally, you can click into your data source to check the sync `Added` the 20 files as expected:\n",
    "\n",
    "<img src=\"images/bedrock-kbs/04b-kb-data-sync-details.png\" width=\"600\" alt=\"Data source details screen showing sync completed successfully with 20 files detected and added to the index, and 0 files failed\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test out the Knowledge Base\n",
    "\n",
    "Before we discuss evaluation at scale, let's run a test queries to check the KB is working properly. Let's go back to the detail page of the knowledge base."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, you can find the knowledge base id is `Z746ERZP5X` in the screenshot below (please check your own *Knowledge Base ID*) on the top of the page in the *Knowledge Base overview* panel.\n",
    "\n",
    "![](images/bedrock-kbs/04c-kb-main-page.png \"Screenshot of the main page of the knowledge base\")\n",
    "\n",
    "👉 **Replace** the below placeholder with your knowledge base's unique ID, and run the cells below to continue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_base_id = \"<TO FILL>\"  # Something like \"Z746ERZP5X\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the ID identified, you can use the Bedrock runtime [RetrieveAndGenerate API](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_agent-runtime_RetrieveAndGenerate.html) to query your knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What kind of economy does Victoria have?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the RetrieveAndGenerate API with Nova Pro model to query the knowledge base\n",
    "rag_resp = bedrock_agent_runtime.retrieve_and_generate(\n",
    "    input={\"text\": query},\n",
    "    retrieveAndGenerateConfiguration={\n",
    "        \"knowledgeBaseConfiguration\": {\n",
    "            \"knowledgeBaseId\": knowledge_base_id,\n",
    "            \"modelArn\": f\"arn:aws:bedrock:us-west-2:{account_id}:inference-profile/us.amazon.nova-pro-v1:0\",\n",
    "        },\n",
    "        \"type\": \"KNOWLEDGE_BASE\",\n",
    "    },\n",
    "    # Optional session ID can help improve results for follow-up questions:\n",
    "    # sessionId='string'\n",
    ")\n",
    "\n",
    "print(\"Plain text response:\")\n",
    "print(\"--------------------\")\n",
    "print(rag_resp[\"output\"][\"text\"], end=\"\\n\\n\\n\")\n",
    "\n",
    "print(\"Full API output:\")\n",
    "print(\"----------------\")\n",
    "rag_resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the full API response from the above cell, the `RetrieveAndGenerate` action provides:\n",
    "\n",
    "- The final text answer\n",
    "- The `retrievedReferences` from the search engine\n",
    "- Specific `citations` localizing which references should be cited by different parts of the text answer\n",
    "\n",
    "\n",
    "It's also possible to run **only the retrieval** through the API, and skip the generative answer synthesis step - as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_resp = bedrock_agent_runtime.retrieve(\n",
    "    knowledgeBaseId=knowledge_base_id,\n",
    "    retrievalQuery={\"text\": query},\n",
    ")\n",
    "print(json.dumps(retrieve_resp[\"retrievalResults\"], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up dataset and metrics for evaluation\n",
    "\n",
    "### Load Dataset\n",
    "\n",
    "For this example, we are going to use a dataset with reference input/output pairs by querying a RAG system and curating the results. See below for instruction on how to fetch your production data from Langfuse.\n",
    "\n",
    "The dataset contains the following columns:\n",
    "\n",
    "- `question`: list[str] - These are the questions your RAG pipeline will be evaluated on.\n",
    "\n",
    "- `contexts`: list[list[str]] - The contexts which were passed into the LLM to answer the question.\n",
    "\n",
    "- `answer`: list[str] - The answer generated from the RAG pipeline and given to the user.\n",
    "\n",
    "- `ground_truths`: list[list[str]] - The ground truth answer to the questions. However, this can be ignored for online evaluations since we will not have access to ground-truth data in our case.\n",
    "\n",
    "For the details of this dataset, please refer to [Exploding Gradients Dataset](https://huggingface.co/datasets/explodinggradients/fiqa/viewer/ragas_eval)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by loading the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "fiqa_eval = load_dataset(\"explodinggradients/fiqa\", \"ragas_eval\")[\"baseline\"]\n",
    "fiqa_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAGAS metrics\n",
    "We're going to measure the following aspects of a RAG system. These metrics are defined in [RAGAS](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/):\n",
    "\n",
    "- [Faithfulness](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/faithfulness/): This measures the factual consistency of the generated answer against the given context.\n",
    "- [Response relevancy](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/answer_relevance/): The ResponseRelevancy metric measures how relevant a response is to the user input.\n",
    "- [Context precision](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/context_precision/): Context Precision is a metric that evaluates whether all of the ground-truth relevant items present in the contexts are ranked high. Ideally all the relevant chunks must appear at the top ranks.\n",
    "\n",
    "Checkout the [RAGAS documentation](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/) to know more about these metrics and how they work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import metrics\n",
    "from ragas.metrics import (\n",
    "    Faithfulness,\n",
    "    ResponseRelevancy,\n",
    "    LLMContextPrecisionWithoutReference,\n",
    ")\n",
    "\n",
    "# metrics you chose\n",
    "metrics = [\n",
    "    Faithfulness(),\n",
    "    ResponseRelevancy(),\n",
    "    LLMContextPrecisionWithoutReference(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.run_config import RunConfig\n",
    "from ragas.metrics.base import MetricWithLLM, MetricWithEmbeddings\n",
    "\n",
    "\n",
    "# util function to init Ragas Metrics\n",
    "def init_ragas_metrics(metrics, llm, embedding):\n",
    "    for metric in metrics:\n",
    "        if isinstance(metric, MetricWithLLM):\n",
    "            print(metric.name + \" llm\")\n",
    "            metric.llm = llm\n",
    "        if isinstance(metric, MetricWithEmbeddings):\n",
    "            print(metric.name + \" embedding\")\n",
    "            metric.embeddings = embedding\n",
    "        run_config = RunConfig()\n",
    "        metric.init(run_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to initialize the metrics with LLMs and embedding models of your choice. In this example we are going to use the Bedrock Nova Pro model and Cohere embedding English model, and use the convenience wrappers from the `langchain-aws` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import BedrockEmbeddings, ChatBedrockConverse\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "\n",
    "config = {\n",
    "    \"region_name\": \"us-west-2\",  # E.g. \"us-east-1\"\n",
    "    \"llm\": \"us.amazon.nova-pro-v1:0\",  # E.g you can also use the claude models \"anthropic.claude-3-5-sonnet-20241022-v2:0\"\n",
    "    \"embeddings\": \"cohere.embed-english-v3\",  # E.g or \"amazon.titan-embed-text-v2:0\"\n",
    "    \"temperature\": 0.4,\n",
    "}\n",
    "\n",
    "evaluator_llm = LangchainLLMWrapper(\n",
    "    ChatBedrockConverse(\n",
    "        region_name=config[\"region_name\"],\n",
    "        base_url=f\"https://bedrock-runtime.{config['region_name']}.amazonaws.com\",\n",
    "        model=config[\"llm\"],\n",
    "        temperature=config[\"temperature\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "evaluator_embeddings = LangchainEmbeddingsWrapper(\n",
    "    BedrockEmbeddings(\n",
    "        region_name=config[\"region_name\"],\n",
    "        model_id=config[\"embeddings\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "init_ragas_metrics(\n",
    "    metrics,\n",
    "    llm=evaluator_llm,\n",
    "    embedding=evaluator_embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trace eval results with Langfuse\n",
    "\n",
    "You can use model-based evaluation with Ragas in 2 ways:\n",
    "1. Score every trace: This means you will run the evaluations for each trace item. This gives you much better idea of how each call made to your RAG pipelines is performing, but please be mindful of the cost.\n",
    "\n",
    "2. Score with sampling: In this method we will take random samples of traces on a periodic basis and score them. This brings down the cost and gives you a rough estimate the performance of your app but may miss out on important samples.\n",
    "\n",
    "In this example, we will demonstrate both solutions using prebuilt dataset and a live RAG pipeline with Bedrock Knowlegebase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score every trace\n",
    "\n",
    "Lets take a small example of a single trace and see how you can score that with Ragas. We first define a utility function to score your trace with the metrics you chose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.dataset_schema import SingleTurnSample\n",
    "\n",
    "\n",
    "async def score_with_ragas(query, chunks, answer, metrics):\n",
    "    scores = {}\n",
    "    for metric in metrics:\n",
    "        sample = SingleTurnSample(\n",
    "            user_input=query,\n",
    "            retrieved_contexts=chunks,\n",
    "            response=answer,\n",
    "        )\n",
    "        print(f\"calculating {metric.name}\")\n",
    "        scores[metric.name] = await metric.single_turn_ascore(sample)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring sample dataset item\n",
    "\n",
    "You compute the score with each request. Below we will go through a dummy application that does the following steps:\n",
    "\n",
    "- Gets a question from the user\n",
    "- Fetch context from the database or vector store that can be used to answer the question from the user\n",
    "- Pass the question and the contexts to the LLM to generate the answer\n",
    "\n",
    "In this case we are demonstrating the use of the Langfuse Python [low-level SDK](https://langfuse.com/docs/sdk/python/low-level-sdk) to log the traces with more granular controls. You can also see an example with the [decorator](https://langfuse.com/docs/sdk/python/decorators) in the later section or read more about them the [langfuse documentation](https://langfuse.com/docs/sdk/overview)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start a new trace when you get a question\n",
    "row = fiqa_eval[0]\n",
    "question = row[\"question\"]\n",
    "contexts = row[\"contexts\"]\n",
    "answer = row[\"answer\"]\n",
    "\n",
    "\n",
    "# Create trace with proper input\n",
    "trace = langfuse.trace(\n",
    "    name=\"rag-fiqa\",\n",
    "    input={\"question\": question},\n",
    "    metadata={\"dataset\": \"fiqa\"}\n",
    ")\n",
    "\n",
    "# Create retrieval span and properly end it\n",
    "retrieval_span = trace.span(\n",
    "    name=\"retrieval\",\n",
    "    input={\"question\": question}\n",
    ")\n",
    "retrieval_span.end(output={\"contexts\": contexts})\n",
    "\n",
    "# Create generation and properly end it\n",
    "generation = trace.generation(\n",
    "    name=\"generation\",\n",
    "    input={\"question\": question, \"contexts\": contexts}\n",
    ")\n",
    "generation.end(output={\"answer\": answer})\n",
    "\n",
    "# End the trace with final output\n",
    "trace.update(output={\"answer\": answer})\n",
    "\n",
    "# compute scores for the question, context, answer tuple\n",
    "ragas_scores = await score_with_ragas(question, contexts, answer, metrics)\n",
    "ragas_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Now you can see this is traced in langfuse but with no score attached, we can check it in the Langfuse UI at:\\n{os.environ['LANGFUSE_HOST']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then attach the scores to the trace by running the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in metrics:\n",
    "      langfuse.score(\n",
    "          name=m.name,\n",
    "          value=ragas_scores[m.name],\n",
    "          trace_id=trace.id\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the score is attached\n",
    "\n",
    "![](images/bedrock-kbs/04e-langfuse-single-eval-trace-score.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring RAG\n",
    "We have already setup the Bedrock Knowledge Base in the first section, we can now **evaluate** the quality of its results against a test dataset - to help us **optimize** the configuration for high quality and low cost.\n",
    "\n",
    "First, let's load the sample dataset of questions, reference answers, and their source documents (to find more of how to prepare this dataset, please see more details in [this github](https://github.com/aws-samples/llm-evaluation-methodology/blob/main/datasets/Prepare-SQuAD.ipynb)):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = pd.read_json(\"datasets/qa.manifest.jsonl\", lines=True)\n",
    "dataset_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Records in this dataset include:\n",
    "\n",
    "- (`doc`) The full text of the source document for this example\n",
    "- (`doc_id`) A unique identifier for the source document\n",
    "- (`question`) The user question to be asked\n",
    "- (`question_id`) A unique identifier for the question\n",
    "- (`answers`) A list of (possibly multiple) reference 'correct' answers, supported by the document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in [Ragas' API Reference](https://docs.ragas.io/en/latest/references/evaluation.html), records in Ragas evaluation datasets typically include:\n",
    "\n",
    "- The `question` that was asked\n",
    "- The `answer` the system generated\n",
    "- The actual text `contexts` the answer was based on (i.e. snippets of document text retrieved by the search engine)\n",
    "- The `ground_truth` answer(s)\n",
    "\n",
    "Here we will integrate [Langfuse Tracking](https://langfuse.com/docs/tracing) into the RAG pipeline with the Langfuse Python SDK using the `@observe()` decorator.\n",
    "\n",
    "We can run an example question through the Bedrock KB retrieve and generate pipeline as shown below, and extract the references ready to calculate metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse.decorators import observe, langfuse_context\n",
    "\n",
    "\n",
    "@observe(name=\"Knowledge Base Retrieve and Generate\")\n",
    "def retrieve_and_generate(\n",
    "    question: str,\n",
    "    kb_id: str,\n",
    "    generate_model_arn: str = f\"arn:aws:bedrock:us-west-2:{account_id}:inference-profile/us.amazon.nova-pro-v1:0\",\n",
    "    **kwargs,\n",
    "):\n",
    "    rag_resp = bedrock_agent_runtime.retrieve_and_generate(\n",
    "        input={\"text\": question},\n",
    "        retrieveAndGenerateConfiguration={\n",
    "            \"knowledgeBaseConfiguration\": {\n",
    "                \"knowledgeBaseId\": kb_id,\n",
    "                \"modelArn\": generate_model_arn,\n",
    "            },\n",
    "            \"type\": \"KNOWLEDGE_BASE\",\n",
    "        },\n",
    "    )\n",
    "    answer = rag_resp[\"output\"][\"text\"]\n",
    "\n",
    "    # Fetch flat list of references from the nested citations -> retrievedReferences:\n",
    "    all_refs = [\n",
    "        r for cite in rag_resp[\"citations\"] for r in cite[\"retrievedReferences\"]\n",
    "    ]\n",
    "    contexts = [r[\"content\"][\"text\"] for r in all_refs]\n",
    "    ref_s3uris = [r[\"location\"][\"s3Location\"][\"uri\"] for r in all_refs]\n",
    "    # Map e.g. 's3://.../doc_id.txt' to 'doc_id':\n",
    "    ref_ids = [uri.rpartition(\"/\")[2].rpartition(\".\")[0] for uri in ref_s3uris]\n",
    "\n",
    "    # Log additional data to the trace\n",
    "    langfuse_context.update_current_observation(\n",
    "        input={\"question\": question, \"contexts\": contexts},\n",
    "        output=answer,\n",
    "        model=\"us.amazon.nova-pro-v1:0\",\n",
    "        session_id=\"kb-rag-session\",\n",
    "        tags=[\"dev\"],\n",
    "        metadata=kwargs,\n",
    "    )\n",
    "\n",
    "    # Get the trace ID for independent scoring\n",
    "    trace_id = langfuse_context.get_current_trace_id()\n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"retrieved_doc_ids\": ref_ids,\n",
    "        \"retrieved_doc_texts\": contexts,\n",
    "        \"trace_id\": trace_id,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run RAG as requests come in and score the results immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse.decorators import observe, langfuse_context\n",
    "from asyncio import run\n",
    "\n",
    "\n",
    "@observe(name=\"Knowledge Base Pipeline\")\n",
    "def rag_pipeline(\n",
    "    question,\n",
    "    user_id: Optional[str] = None,\n",
    "    session_id: Optional[str] = None,\n",
    "    kb_id: Optional[str] = None,\n",
    "    metrics: Optional[Any] = None,\n",
    "):\n",
    "\n",
    "    generated_answer = retrieve_and_generate(\n",
    "        question=question,\n",
    "        kb_id=kb_id,\n",
    "        kwargs={\"database\": \"Bedrock Knowledge Base\", \"kb_id\": kb_id},\n",
    "    )\n",
    "    contexts = generated_answer[\"retrieved_doc_texts\"]\n",
    "    answer = generated_answer[\"answer\"]\n",
    "    trace_id = generated_answer[\"trace_id\"]\n",
    "\n",
    "    score = run(score_with_ragas(question, contexts, answer=answer, metrics=metrics))\n",
    "    langfuse_context.update_current_trace(\n",
    "        user_id=user_id,\n",
    "        session_id=session_id,\n",
    "        tags=[\"dev\"],\n",
    "    )\n",
    "    for s in score:\n",
    "        langfuse.score(name=s, value=score[s], trace_id=trace_id)\n",
    "    return generated_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag_pipeline(dataset_df.iloc[0][\"question\"], kb_id=knowledge_base_id, metrics=metrics)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring with sampling\n",
    "\n",
    "Scoring every production trace can be time-consuming and costly depending on your application architecture and traffic. In that case, it's better to start off with a sampling method. Decide a timespan you want to run the batch process and the number of traces you want to sample from that time slice. Create a dataset and call ragas.evaluate to analyze the result.\n",
    "\n",
    "You can run this periodically to keep track of how the scores are changing across timeslices and figure out if there are any discrepancies.\n",
    "\n",
    "We will evaluate the existing results generated previously by the `retrieve_and_generate()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate 10 production traces by running RAG on the first 10 questions in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_generated_outputs = [\n",
    "    retrieve_and_generate(\n",
    "        question=rec.question,\n",
    "        kb_id=knowledge_base_id,\n",
    "        kwargs={\"database\": \"Bedrock Knowledge Base\", \"kb_id\": knowledge_base_id},\n",
    "    )\n",
    "    for _, rec in dataset_df.head(10).iterrows()\n",
    "]\n",
    "rag_generated_outputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the results are uploaded to langfuse you can retrieve it as needed with this handy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse.api.resources.commons.types.trace_with_details import TraceWithDetails\n",
    "\n",
    "def get_traces(\n",
    "    limit: int = 5,\n",
    "    name: Optional[str] = None,\n",
    "    user_id: Optional[str] = None,\n",
    "    session_id: Optional[str] = None,\n",
    "    from_timestamp: Optional[str] = None,\n",
    "    to_timestamp: Optional[str] = None,\n",
    ") -> List[TraceWithDetails]:\n",
    "    \"\"\"Query Langfuse for traces matching the given filters.\n",
    "    See https://langfuse.com/docs/query-traces for more details.\"\"\"\n",
    "\n",
    "    all_data = []\n",
    "    page = 1\n",
    "\n",
    "    while True:\n",
    "        response = langfuse.fetch_traces(\n",
    "            page=page,\n",
    "            name=name,\n",
    "            user_id=user_id,\n",
    "            session_id=session_id,\n",
    "            from_timestamp=from_timestamp,\n",
    "            to_timestamp=to_timestamp,\n",
    "        )\n",
    "        if not response.data:\n",
    "            break\n",
    "        page += 1\n",
    "        all_data.extend(response.data)\n",
    "        if len(all_data) > limit:\n",
    "            break\n",
    "\n",
    "    return all_data[:limit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "NUM_TRACES_TO_SAMPLE = 3\n",
    "traces = get_traces(name=\"Knowledge Base Retrieve and Generate\", limit=10)\n",
    "if len(traces) > NUM_TRACES_TO_SAMPLE:\n",
    "    traces_sample = sample(traces, NUM_TRACES_TO_SAMPLE)\n",
    "else:\n",
    "    traces_sample = traces\n",
    "\n",
    "print(f\"Sampled {len(traces_sample)} traces from {len(traces)} filtered traces\")\n",
    "for trace in traces_sample:\n",
    "    print(f\"Trace ID: {trace.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets make a batch and score it. Ragas uses huggingface dataset object to build the dataset and run the evaluation. If you run this on your own production data, use the right keys to extract the question, contexts and answer from the trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score on a sample\n",
    "evaluation_batch = {\n",
    "    \"question\": [],\n",
    "    \"contexts\": [],\n",
    "    \"answer\": [],\n",
    "    \"trace_id\": [],\n",
    "}\n",
    "\n",
    "for sample in traces_sample:\n",
    "    evaluation_batch[\"question\"].append(sample.input[\"question\"])\n",
    "    evaluation_batch[\"contexts\"].append(sample.input[\"contexts\"])\n",
    "    evaluation_batch[\"answer\"].append(sample.output)\n",
    "    evaluation_batch[\"trace_id\"].append(sample.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the ragas evaluate function to score an entire dataset instead of single turn. See [Ragas evaluate](https://docs.ragas.io/en/latest/references/evaluate/) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run ragas evaluate\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import Faithfulness, ResponseRelevancy\n",
    "\n",
    "ds = Dataset.from_dict(evaluation_batch)\n",
    "evals_results = evaluate(\n",
    "    ds,\n",
    "    llm=evaluator_llm,\n",
    "    embeddings=evaluator_embeddings,\n",
    "    metrics=[Faithfulness(), ResponseRelevancy()],\n",
    ")\n",
    "evals_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that is it! You can see the scores over a time period. Let's render the results in a dataframe to see the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = evals_results.to_pandas()\n",
    "\n",
    "# add the langfuse trace_id to the result dataframe\n",
    "df[\"trace_id\"] = ds[\"trace_id\"]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also push the scores back into Langfuse and attach them to the traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in df.iterrows():\n",
    "    for metric_name in [\"faithfulness\", \"answer_relevancy\"]:\n",
    "        langfuse.score(\n",
    "            name=metric_name, value=row[metric_name], trace_id=row[\"trace_id\"]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now go back to the Langfuse console and check the updated scores in the traces.\n",
    "\n",
    "![](images/bedrock-kbs/score-with-sampling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratuations!\n",
    "You have successfully finished Lab 2.\n",
    "\n",
    "If you are at an AWS event, you can return to the workshop studio for additional instructions before moving into the next lab, where we will explore model-based evaluation and guardrails."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
